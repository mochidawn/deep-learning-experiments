{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"1\"\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, Dense, merge, Flatten, Dropout, Lambda\n",
    "from keras.layers import BatchNormalization, Merge, Activation, GlobalAveragePooling2D\n",
    "from keras.layers import Multiply, Add\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import cifar10\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "def get_data(add_noise):\n",
    "    \"\"\"\n",
    "    add_noise: add gaussian noise to both training and testing images\n",
    "    --> We want to know if attention is helpful or not\n",
    "    \"\"\"\n",
    "    (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "    y_train = to_categorical(y_train)\n",
    "    y_test = to_categorical(y_test)\n",
    "\n",
    "    # Confirm the dimension\n",
    "    if len(X_train.shape) == 3:\n",
    "        # in gray-scale\n",
    "        X_train = np.expand_dims(X_train, axis = -1)\n",
    "        X_test = np.expand_dims(X_test, axis = -1)\n",
    "        \n",
    "    if add_noise:\n",
    "        aug_seq = iaa.Sequential([iaa.AdditiveGaussianNoise(scale=0.2 * 255)])\n",
    "        X_train = aug_seq.augment_images(X_train)\n",
    "        X_test = aug_seq.augment_images(X_test)\n",
    "        \n",
    "    X_train = X_train / 255.\n",
    "    X_test = X_test / 255.\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(attention):\n",
    "\n",
    "    inputs = Input(shape= X_train.shape[1:],\n",
    "                   name='input')\n",
    "\n",
    "    conv_1a = Convolution2D(filters=64, \n",
    "                            kernel_size=(3, 3),\n",
    "                            padding = 'same',\n",
    "                            name='conv_1')(inputs)\n",
    "    conv_1a = BatchNormalization(axis = -1)(conv_1a)\n",
    "    conv_1a = Activation('relu')(conv_1a)\n",
    "    conv_1a = MaxPooling2D(pool_size=(2,2), padding='same')(conv_1a)\n",
    "\n",
    "    conv_2a = Convolution2D(filters = 128, \n",
    "                            kernel_size=(3,3),\n",
    "                            padding = 'same',\n",
    "                            name='conv_2')(conv_1a)\n",
    "    conv_2a = BatchNormalization(axis = -1)(conv_2a)\n",
    "    conv_2a = Activation('relu')(conv_2a)\n",
    "    #conv_2a = MaxPooling2D(pool_size=(2,2))(conv_2a)\n",
    "\n",
    "    if attention:\n",
    "        # Create a pixel weighted layer\n",
    "        ## To make the attention focus on the spot, should i add total-variation loss on att_ ?\n",
    "        att_ = Convolution2D(filters=1, kernel_size=(1,1), padding='same', activation='sigmoid')(conv_2a) # tanh or sigmoid?\n",
    "        att_ = Multiply()([conv_2a, att_])\n",
    "    else:\n",
    "        att_ = Lambda(lambda x: x * 1)(conv_2a) # make conv_2a multiply with ones...\n",
    "        att_ = Multiply()([conv_2a, att_])\n",
    "\n",
    "    conv_2a = MaxPooling2D(pool_size=(2,2))(att_)\n",
    "\n",
    "    conv_3a = Convolution2D(filters=256, \n",
    "                            kernel_size=(3,3), \n",
    "                            padding='same')(conv_2a)\n",
    "    conv_3a = BatchNormalization(axis = -1)(conv_3a)\n",
    "    conv_3a = Activation('relu')(conv_3a)\n",
    "\n",
    "    gap = GlobalAveragePooling2D()(conv_3a)\n",
    "    out = Dense(10, activation='softmax')(gap)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=out)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An experiment begin\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv_1 (Conv2D)                 (None, 32, 32, 64)   1792        input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32, 32, 64)   256         conv_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 32, 32, 64)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 16, 16, 64)   0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_2 (Conv2D)                 (None, 16, 16, 128)  73856       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 16, 16, 128)  512         conv_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 16, 16, 128)  0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 16, 16, 128)  0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 16, 16, 128)  0           activation_2[0][0]               \n",
      "                                                                 lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 8, 8, 128)    0           multiply_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 8, 8, 256)    295168      max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 8, 8, 256)    1024        conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 8, 8, 256)    0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 256)          0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10)           2570        global_average_pooling2d_1[0][0] \n",
      "==================================================================================================\n",
      "Total params: 375,178\n",
      "Trainable params: 374,282\n",
      "Non-trainable params: 896\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 1.5436 - acc: 0.4540Epoch 00001: val_loss improved from inf to 1.41017, saving model to ./model_noise0_att0.h5\n",
      "782/782 [==============================] - 35s 44ms/step - loss: 1.5431 - acc: 0.4541 - val_loss: 1.4102 - val_acc: 0.4918\n",
      "Epoch 2/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 1.2753 - acc: 0.5507Epoch 00002: val_loss improved from 1.41017 to 1.19349, saving model to ./model_noise0_att0.h5\n",
      "782/782 [==============================] - 31s 40ms/step - loss: 1.2753 - acc: 0.5506 - val_loss: 1.1935 - val_acc: 0.5761\n",
      "Epoch 3/200\n",
      "778/782 [============================>.] - ETA: 0s - loss: 1.1821 - acc: 0.5852Epoch 00003: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 1.1819 - acc: 0.5850 - val_loss: 1.1950 - val_acc: 0.5799\n",
      "Epoch 4/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 1.1157 - acc: 0.6080Epoch 00004: val_loss improved from 1.19349 to 1.15421, saving model to ./model_noise0_att0.h5\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 1.1156 - acc: 0.6081 - val_loss: 1.1542 - val_acc: 0.5896\n",
      "Epoch 5/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 1.0667 - acc: 0.6287Epoch 00005: val_loss did not improve\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 1.0667 - acc: 0.6286 - val_loss: 1.1875 - val_acc: 0.5739\n",
      "Epoch 6/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 1.0338 - acc: 0.6389Epoch 00006: val_loss improved from 1.15421 to 1.02753, saving model to ./model_noise0_att0.h5\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 1.0338 - acc: 0.6389 - val_loss: 1.0275 - val_acc: 0.6348\n",
      "Epoch 7/200\n",
      "778/782 [============================>.] - ETA: 0s - loss: 1.0013 - acc: 0.6507Epoch 00007: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 1.0013 - acc: 0.6508 - val_loss: 1.1555 - val_acc: 0.6095\n",
      "Epoch 8/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.9734 - acc: 0.6609- ETA: 4s - losEpoch 00008: val_loss improved from 1.02753 to 0.99815, saving model to ./model_noise0_att0.h5\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.9732 - acc: 0.6610 - val_loss: 0.9982 - val_acc: 0.6501\n",
      "Epoch 9/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.9495 - acc: 0.6694Epoch 00009: val_loss did not improve\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 0.9499 - acc: 0.6693 - val_loss: 1.0211 - val_acc: 0.6375\n",
      "Epoch 10/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.9321 - acc: 0.6749Epoch 00010: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.9319 - acc: 0.6748 - val_loss: 1.0149 - val_acc: 0.6492\n",
      "Epoch 11/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.9105 - acc: 0.6849- ETA: 2s -Epoch 00011: val_loss improved from 0.99815 to 0.86059, saving model to ./model_noise0_att0.h5\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.9106 - acc: 0.6848 - val_loss: 0.8606 - val_acc: 0.6980\n",
      "Epoch 12/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.8943 - acc: 0.6886Epoch 00012: val_loss did not improve\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 0.8946 - acc: 0.6886 - val_loss: 0.9578 - val_acc: 0.6681\n",
      "Epoch 13/200\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.8814 - acc: 0.6928Epoch 00013: val_loss did not improve\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 0.8813 - acc: 0.6930 - val_loss: 0.9551 - val_acc: 0.6711\n",
      "Epoch 14/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.8643 - acc: 0.6992Epoch 00014: val_loss did not improve\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 0.8644 - acc: 0.6991 - val_loss: 0.9066 - val_acc: 0.6803\n",
      "Epoch 15/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.8513 - acc: 0.7082- ETA: 3s - loss: 0.8502 - accEpoch 00015: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.8512 - acc: 0.7082 - val_loss: 0.9116 - val_acc: 0.6779\n",
      "Epoch 16/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.8386 - acc: 0.7095Epoch 00016: val_loss did not improve\n",
      "782/782 [==============================] - 31s 40ms/step - loss: 0.8387 - acc: 0.7096 - val_loss: 0.9032 - val_acc: 0.6795\n",
      "Epoch 17/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "781/782 [============================>.] - ETA: 0s - loss: 0.8293 - acc: 0.7120\n",
      "Epoch 00017: reducing learning rate to 4.999999873689376e-05.\n",
      "Epoch 00017: val_loss did not improve\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 0.8291 - acc: 0.7121 - val_loss: 1.0122 - val_acc: 0.6492\n",
      "Epoch 18/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.7990 - acc: 0.7236Epoch 00018: val_loss improved from 0.86059 to 0.77884, saving model to ./model_noise0_att0.h5\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.7991 - acc: 0.7237 - val_loss: 0.7788 - val_acc: 0.7334\n",
      "Epoch 19/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.7879 - acc: 0.7278Epoch 00019: val_loss improved from 0.77884 to 0.74104, saving model to ./model_noise0_att0.h5\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.7877 - acc: 0.7278 - val_loss: 0.7410 - val_acc: 0.7420\n",
      "Epoch 20/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.7817 - acc: 0.7297- ETA: 2s - loss: 0.7834 - acc: 0. - ETA: Epoch 00020: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.7816 - acc: 0.7297 - val_loss: 0.8090 - val_acc: 0.7254\n",
      "Epoch 21/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.7738 - acc: 0.7342Epoch 00021: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.7737 - acc: 0.7343 - val_loss: 0.7944 - val_acc: 0.7212\n",
      "Epoch 22/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.7708 - acc: 0.7336Epoch 00022: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.7702 - acc: 0.7338 - val_loss: 0.7552 - val_acc: 0.7386\n",
      "Epoch 23/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.7633 - acc: 0.7372Epoch 00023: val_loss did not improve\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 0.7636 - acc: 0.7372 - val_loss: 0.7482 - val_acc: 0.7389\n",
      "Epoch 24/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.7652 - acc: 0.7374Epoch 00024: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.7650 - acc: 0.7375 - val_loss: 0.7624 - val_acc: 0.7364\n",
      "Epoch 25/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.7553 - acc: 0.7403Epoch 00025: val_loss improved from 0.74104 to 0.71285, saving model to ./model_noise0_att0.h5\n",
      "782/782 [==============================] - 31s 40ms/step - loss: 0.7552 - acc: 0.7405 - val_loss: 0.7129 - val_acc: 0.7559\n",
      "Epoch 26/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.7461 - acc: 0.7421Epoch 00026: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.7463 - acc: 0.7420 - val_loss: 0.7646 - val_acc: 0.7293\n",
      "Epoch 27/200\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.7469 - acc: 0.7415- ETA: 3s - loss:  - ETA: 2s Epoch 00027: val_loss improved from 0.71285 to 0.68753, saving model to ./model_noise0_att0.h5\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.7467 - acc: 0.7417 - val_loss: 0.6875 - val_acc: 0.7652\n",
      "Epoch 28/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.7434 - acc: 0.7452Epoch 00028: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.7435 - acc: 0.7451 - val_loss: 0.6943 - val_acc: 0.7616\n",
      "Epoch 29/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.7333 - acc: 0.7462Epoch 00029: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.7331 - acc: 0.7464 - val_loss: 0.7452 - val_acc: 0.7376\n",
      "Epoch 30/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.7356 - acc: 0.7465- ETA: 4s - loss: 0.732Epoch 00030: val_loss did not improve\n",
      "782/782 [==============================] - 29s 38ms/step - loss: 0.7353 - acc: 0.7465 - val_loss: 0.7493 - val_acc: 0.7406\n",
      "Epoch 31/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.7316 - acc: 0.7491Epoch 00031: val_loss did not improve\n",
      "782/782 [==============================] - 29s 38ms/step - loss: 0.7314 - acc: 0.7491 - val_loss: 0.7165 - val_acc: 0.7502\n",
      "Epoch 32/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.7257 - acc: 0.7510Epoch 00032: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.7258 - acc: 0.7509 - val_loss: 0.7027 - val_acc: 0.7558\n",
      "Epoch 33/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.7247 - acc: 0.7486\n",
      "Epoch 00033: reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 00033: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.7246 - acc: 0.7486 - val_loss: 0.6941 - val_acc: 0.7578\n",
      "Epoch 34/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.7043 - acc: 0.7580Epoch 00034: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.7046 - acc: 0.7580 - val_loss: 0.6882 - val_acc: 0.7644\n",
      "Epoch 35/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.7062 - acc: 0.7580Epoch 00035: val_loss improved from 0.68753 to 0.66071, saving model to ./model_noise0_att0.h5\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.7061 - acc: 0.7580 - val_loss: 0.6607 - val_acc: 0.7698\n",
      "Epoch 36/200\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.6970 - acc: 0.7609Epoch 00036: val_loss improved from 0.66071 to 0.65198, saving model to ./model_noise0_att0.h5\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.6968 - acc: 0.7610 - val_loss: 0.6520 - val_acc: 0.7728\n",
      "Epoch 37/200\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.6951 - acc: 0.7628Epoch 00037: val_loss did not improve\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 0.6952 - acc: 0.7626 - val_loss: 0.6614 - val_acc: 0.7702\n",
      "Epoch 38/200\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.6988 - acc: 0.7589Epoch 00038: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.6986 - acc: 0.7589 - val_loss: 0.7129 - val_acc: 0.7560\n",
      "Epoch 39/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.6951 - acc: 0.7619Epoch 00039: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.6952 - acc: 0.7618 - val_loss: 0.6675 - val_acc: 0.7667\n",
      "Epoch 40/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.6925 - acc: 0.7623Epoch 00040: val_loss did not improve\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 0.6924 - acc: 0.7623 - val_loss: 0.6650 - val_acc: 0.7714\n",
      "Epoch 41/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.6871 - acc: 0.7647Epoch 00041: val_loss did not improve\n",
      "782/782 [==============================] - 29s 38ms/step - loss: 0.6870 - acc: 0.7648 - val_loss: 0.6674 - val_acc: 0.7672\n",
      "Epoch 42/200\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.6862 - acc: 0.7633\n",
      "Epoch 00042: reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 00042: val_loss did not improve\n",
      "782/782 [==============================] - 29s 37ms/step - loss: 0.6863 - acc: 0.7634 - val_loss: 0.6791 - val_acc: 0.7630\n",
      "Epoch 43/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.6798 - acc: 0.7663Epoch 00043: val_loss improved from 0.65198 to 0.64391, saving model to ./model_noise0_att0.h5\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.6797 - acc: 0.7663 - val_loss: 0.6439 - val_acc: 0.7780\n",
      "Epoch 44/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.6767 - acc: 0.7697- ETA: 1s - loss: 0Epoch 00044: val_loss improved from 0.64391 to 0.63700, saving model to ./model_noise0_att0.h5\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 0.6769 - acc: 0.7697 - val_loss: 0.6370 - val_acc: 0.7775\n",
      "Epoch 45/200\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.6777 - acc: 0.7677Epoch 00045: val_loss did not improve\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 0.6775 - acc: 0.7678 - val_loss: 0.6421 - val_acc: 0.7795\n",
      "Epoch 46/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "780/782 [============================>.] - ETA: 0s - loss: 0.6757 - acc: 0.7673- ETA: 0s - loss: 0.6754 - acc: 0Epoch 00046: val_loss improved from 0.63700 to 0.63147, saving model to ./model_noise0_att0.h5\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.6754 - acc: 0.7674 - val_loss: 0.6315 - val_acc: 0.7843\n",
      "Epoch 47/200\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.6757 - acc: 0.7680- ETA: 5s - loss: 0.6730 - acc: 0.7 - ETA: 5s - loss: 0.672Epoch 00047: val_loss improved from 0.63147 to 0.62681, saving model to ./model_noise0_att0.h5\n",
      "782/782 [==============================] - 31s 40ms/step - loss: 0.6757 - acc: 0.7681 - val_loss: 0.6268 - val_acc: 0.7845\n",
      "Epoch 48/200\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.6765 - acc: 0.7684- ETA: 3s - loss: 0.6755 - acc: - ETAEpoch 00048: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.6765 - acc: 0.7683 - val_loss: 0.6346 - val_acc: 0.7820\n",
      "Epoch 49/200\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.6765 - acc: 0.7691Epoch 00049: val_loss did not improve\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 0.6766 - acc: 0.7690 - val_loss: 0.6317 - val_acc: 0.7813\n",
      "Epoch 50/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.6745 - acc: 0.7681- ETA: 0s - loss: 0.6743 - acc: 0.Epoch 00050: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.6747 - acc: 0.7681 - val_loss: 0.6397 - val_acc: 0.7779\n",
      "Epoch 51/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.6735 - acc: 0.7688Epoch 00051: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.6732 - acc: 0.7689 - val_loss: 0.6376 - val_acc: 0.7795\n",
      "Epoch 52/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.6720 - acc: 0.7702Epoch 00052: val_loss did not improve\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 0.6720 - acc: 0.7701 - val_loss: 0.6396 - val_acc: 0.7791\n",
      "Epoch 53/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.6694 - acc: 0.7707- ETA: 1s - loss: 0.6702 - acc: - ETA: 1s - loss: 0.6692 -\n",
      "Epoch 00053: reducing learning rate to 6.24999984211172e-06.\n",
      "Epoch 00053: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.6692 - acc: 0.7708 - val_loss: 0.6346 - val_acc: 0.7806\n",
      "Epoch 54/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.6653 - acc: 0.7714Epoch 00054: val_loss improved from 0.62681 to 0.62319, saving model to ./model_noise0_att0.h5\n",
      "782/782 [==============================] - 29s 38ms/step - loss: 0.6653 - acc: 0.7714 - val_loss: 0.6232 - val_acc: 0.7852\n",
      "Epoch 55/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.6639 - acc: 0.7737Epoch 00055: val_loss did not improve\n",
      "782/782 [==============================] - 31s 40ms/step - loss: 0.6638 - acc: 0.7737 - val_loss: 0.6268 - val_acc: 0.7841\n",
      "Epoch 56/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.6609 - acc: 0.7755Epoch 00056: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.6608 - acc: 0.7756 - val_loss: 0.6259 - val_acc: 0.7836\n",
      "Epoch 57/200\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.6643 - acc: 0.7729- ETA: 1s - loss: 0.663Epoch 00057: val_loss improved from 0.62319 to 0.62214, saving model to ./model_noise0_att0.h5\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.6650 - acc: 0.7725 - val_loss: 0.6221 - val_acc: 0.7822\n",
      "Epoch 58/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.6609 - acc: 0.7716Epoch 00058: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.6612 - acc: 0.7714 - val_loss: 0.6268 - val_acc: 0.7831\n",
      "Epoch 59/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.6598 - acc: 0.7733Epoch 00059: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.6599 - acc: 0.7732 - val_loss: 0.6256 - val_acc: 0.7850\n",
      "Epoch 60/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.6615 - acc: 0.7734- ETA: 0s - loss: 0.6616 - acEpoch 00060: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.6615 - acc: 0.7735 - val_loss: 0.6225 - val_acc: 0.7843\n",
      "Epoch 61/200\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.6597 - acc: 0.7734- ETA: 0s - loss: 0.6596 - acc: Epoch 00061: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.6594 - acc: 0.7736 - val_loss: 0.6250 - val_acc: 0.7864\n",
      "Epoch 62/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.6611 - acc: 0.7734- ETA: 2Epoch 00062: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.6610 - acc: 0.7735 - val_loss: 0.6259 - val_acc: 0.7849\n",
      "Epoch 63/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.6567 - acc: 0.7748- ETA: 1s - loss: 0.6558 - acc: - ETA: 1s - loss: 0.6565 - ETA: 0s - loss: 0.6569 - acc: 0.774Epoch 00063: val_loss improved from 0.62214 to 0.61831, saving model to ./model_noise0_att0.h5\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.6569 - acc: 0.7746 - val_loss: 0.6183 - val_acc: 0.7857\n",
      "Epoch 64/200\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.6597 - acc: 0.7740Epoch 00064: val_loss improved from 0.61831 to 0.61694, saving model to ./model_noise0_att0.h5\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.6600 - acc: 0.7739 - val_loss: 0.6169 - val_acc: 0.7861\n",
      "Epoch 65/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.6570 - acc: 0.7750Epoch 00065: val_loss did not improve\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 0.6571 - acc: 0.7748 - val_loss: 0.6190 - val_acc: 0.7862\n",
      "Epoch 66/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.6592 - acc: 0.7739Epoch 00066: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.6593 - acc: 0.7738 - val_loss: 0.6222 - val_acc: 0.7850\n",
      "Epoch 67/200\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.6558 - acc: 0.7753Epoch 00067: val_loss improved from 0.61694 to 0.61259, saving model to ./model_noise0_att0.h5\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.6557 - acc: 0.7751 - val_loss: 0.6126 - val_acc: 0.7895\n",
      "Epoch 68/200\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.6608 - acc: 0.7757Epoch 00068: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.6608 - acc: 0.7758 - val_loss: 0.6247 - val_acc: 0.7850\n",
      "Epoch 69/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.6580 - acc: 0.7761Epoch 00069: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.6584 - acc: 0.7760 - val_loss: 0.6195 - val_acc: 0.7852\n",
      "Epoch 70/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.6582 - acc: 0.7744Epoch 00070: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.6582 - acc: 0.7744 - val_loss: 0.6147 - val_acc: 0.7885\n",
      "Epoch 71/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.6574 - acc: 0.7750Epoch 00071: val_loss did not improve\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 0.6574 - acc: 0.7749 - val_loss: 0.6191 - val_acc: 0.7875\n",
      "Epoch 72/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.6565 - acc: 0.7754Epoch 00072: val_loss did not improve\n",
      "782/782 [==============================] - 31s 40ms/step - loss: 0.6569 - acc: 0.7753 - val_loss: 0.6183 - val_acc: 0.7840\n",
      "Epoch 73/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.6572 - acc: 0.7767\n",
      "Epoch 00073: reducing learning rate to 3.12499992105586e-06.\n",
      "Epoch 00073: val_loss did not improve\n",
      "782/782 [==============================] - 31s 40ms/step - loss: 0.6569 - acc: 0.7769 - val_loss: 0.6180 - val_acc: 0.7873\n",
      "Epoch 74/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.6498 - acc: 0.7775Epoch 00074: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.6497 - acc: 0.7774 - val_loss: 0.6141 - val_acc: 0.7889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.6514 - acc: 0.7767Epoch 00075: val_loss did not improve\n",
      "782/782 [==============================] - 29s 37ms/step - loss: 0.6512 - acc: 0.7769 - val_loss: 0.6150 - val_acc: 0.7879\n",
      "Epoch 76/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.6531 - acc: 0.7773- ETA: 1s - loss: 0.6Epoch 00076: val_loss did not improve\n",
      "782/782 [==============================] - 31s 40ms/step - loss: 0.6532 - acc: 0.7773 - val_loss: 0.6134 - val_acc: 0.7859\n",
      "Epoch 77/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.6536 - acc: 0.7778- ETA:Epoch 00077: val_loss improved from 0.61259 to 0.61056, saving model to ./model_noise0_att0.h5\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.6534 - acc: 0.7779 - val_loss: 0.6106 - val_acc: 0.7898\n",
      "Epoch 78/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.6513 - acc: 0.7774Epoch 00078: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.6513 - acc: 0.7775 - val_loss: 0.6131 - val_acc: 0.7879\n",
      "Epoch 79/200\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.6500 - acc: 0.7779- ETA: 0s - loss: 0.6503 - acc: Epoch 00079: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.6502 - acc: 0.7779 - val_loss: 0.6108 - val_acc: 0.7899\n",
      "Epoch 80/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.6483 - acc: 0.7793Epoch 00080: val_loss improved from 0.61056 to 0.60775, saving model to ./model_noise0_att0.h5\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.6482 - acc: 0.7793 - val_loss: 0.6077 - val_acc: 0.7901\n",
      "Epoch 81/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.6504 - acc: 0.7776Epoch 00081: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.6506 - acc: 0.7777 - val_loss: 0.6106 - val_acc: 0.7882\n",
      "Epoch 82/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.6508 - acc: 0.7763Epoch 00082: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.6509 - acc: 0.7763 - val_loss: 0.6118 - val_acc: 0.7893\n",
      "Epoch 83/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.6503 - acc: 0.7767- ETA: 1s - loss: 0.6Epoch 00083: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.6501 - acc: 0.7769 - val_loss: 0.6126 - val_acc: 0.7887\n",
      "Epoch 84/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.6506 - acc: 0.7784Epoch 00084: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.6505 - acc: 0.7784 - val_loss: 0.6116 - val_acc: 0.7884\n",
      "Epoch 85/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.6551 - acc: 0.7743Epoch 00085: val_loss did not improve\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 0.6547 - acc: 0.7744 - val_loss: 0.6130 - val_acc: 0.7880\n",
      "Epoch 86/200\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.6494 - acc: 0.7749- E\n",
      "Epoch 00086: reducing learning rate to 1.56249996052793e-06.\n",
      "Epoch 00086: val_loss did not improve\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 0.6494 - acc: 0.7749 - val_loss: 0.6112 - val_acc: 0.7893\n",
      "Epoch 87/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.6455 - acc: 0.7791- ETA: 1s - loss: Epoch 00087: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.6454 - acc: 0.7791 - val_loss: 0.6078 - val_acc: 0.7899\n",
      "Epoch 88/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.6489 - acc: 0.7785Epoch 00088: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.6487 - acc: 0.7786 - val_loss: 0.6101 - val_acc: 0.7898\n",
      "Epoch 89/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.6487 - acc: 0.7780Epoch 00089: val_loss did not improve\n",
      "782/782 [==============================] - 29s 38ms/step - loss: 0.6489 - acc: 0.7779 - val_loss: 0.6087 - val_acc: 0.7905\n",
      "Epoch 90/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.6500 - acc: 0.7793Epoch 00090: val_loss improved from 0.60775 to 0.60768, saving model to ./model_noise0_att0.h5\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.6499 - acc: 0.7793 - val_loss: 0.6077 - val_acc: 0.7914\n",
      "Epoch 91/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.6476 - acc: 0.7802- ETA: 1s - los\n",
      "Epoch 00091: reducing learning rate to 7.81249980263965e-07.\n",
      "Epoch 00091: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.6475 - acc: 0.7803 - val_loss: 0.6117 - val_acc: 0.7902\n",
      "Epoch 92/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.6477 - acc: 0.7787- ETA: 1s - losEpoch 00092: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.6477 - acc: 0.7787 - val_loss: 0.6080 - val_acc: 0.7896\n",
      "Epoch 93/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.6490 - acc: 0.7780Epoch 00093: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.6489 - acc: 0.7780 - val_loss: 0.6084 - val_acc: 0.7897\n",
      "Epoch 94/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.6462 - acc: 0.7793Epoch 00094: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.6464 - acc: 0.7793 - val_loss: 0.6091 - val_acc: 0.7894\n",
      "Epoch 95/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.6462 - acc: 0.7789Epoch 00095: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.6460 - acc: 0.7790 - val_loss: 0.6088 - val_acc: 0.7899\n",
      "Epoch 96/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.6498 - acc: 0.7779\n",
      "Epoch 00096: reducing learning rate to 3.906249901319825e-07.\n",
      "Epoch 00096: val_loss did not improve\n",
      "782/782 [==============================] - 31s 40ms/step - loss: 0.6502 - acc: 0.7779 - val_loss: 0.6079 - val_acc: 0.7900\n",
      "Epoch 97/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.6505 - acc: 0.7778Epoch 00097: val_loss improved from 0.60768 to 0.60747, saving model to ./model_noise0_att0.h5\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 0.6504 - acc: 0.7778 - val_loss: 0.6075 - val_acc: 0.7904\n",
      "Epoch 98/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.6462 - acc: 0.7786Epoch 00098: val_loss did not improve\n",
      "782/782 [==============================] - 31s 40ms/step - loss: 0.6460 - acc: 0.7786 - val_loss: 0.6083 - val_acc: 0.7897\n",
      "Epoch 99/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.6489 - acc: 0.7770Epoch 00099: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.6488 - acc: 0.7770 - val_loss: 0.6083 - val_acc: 0.7902\n",
      "Epoch 100/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.6467 - acc: 0.7790Epoch 00100: val_loss did not improve\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 0.6465 - acc: 0.7792 - val_loss: 0.6081 - val_acc: 0.7900\n",
      "Epoch 101/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.6467 - acc: 0.7777Epoch 00101: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.6467 - acc: 0.7778 - val_loss: 0.6087 - val_acc: 0.7900\n",
      "Epoch 102/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.6480 - acc: 0.7774Epoch 00102: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.6479 - acc: 0.7774 - val_loss: 0.6089 - val_acc: 0.7893\n",
      "Epoch 103/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.6463 - acc: 0.7792\n",
      "Epoch 00103: reducing learning rate to 1.9531249506599124e-07.\n",
      "Epoch 00103: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.6465 - acc: 0.7792 - val_loss: 0.6082 - val_acc: 0.7893\n",
      "Epoch 104/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.6459 - acc: 0.7784Epoch 00104: val_loss did not improve\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 0.6459 - acc: 0.7784 - val_loss: 0.6080 - val_acc: 0.7899\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 105/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.6452 - acc: 0.7801Epoch 00105: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.6451 - acc: 0.7802 - val_loss: 0.6081 - val_acc: 0.7894\n",
      "Epoch 106/200\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.6449 - acc: 0.7799Epoch 00106: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.6455 - acc: 0.7797 - val_loss: 0.6081 - val_acc: 0.7892\n",
      "Epoch 107/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.6489 - acc: 0.7769Epoch 00107: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.6489 - acc: 0.7769 - val_loss: 0.6081 - val_acc: 0.7899\n",
      "An experiment done\n",
      "An experiment begin\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv_1 (Conv2D)                 (None, 32, 32, 64)   1792        input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32, 32, 64)   256         conv_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 32, 32, 64)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 16, 16, 64)   0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_2 (Conv2D)                 (None, 16, 16, 128)  73856       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 16, 16, 128)  512         conv_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 16, 16, 128)  0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 16, 16, 1)    129         activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 16, 16, 128)  0           activation_2[0][0]               \n",
      "                                                                 conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 8, 8, 128)    0           multiply_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 8, 8, 256)    295168      max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 8, 8, 256)    1024        conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 8, 8, 256)    0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 256)          0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10)           2570        global_average_pooling2d_1[0][0] \n",
      "==================================================================================================\n",
      "Total params: 375,307\n",
      "Trainable params: 374,411\n",
      "Non-trainable params: 896\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 1.5510 - acc: 0.4498Epoch 00001: val_loss improved from inf to 1.43546, saving model to ./model_noise0_att1.h5\n",
      "782/782 [==============================] - 31s 40ms/step - loss: 1.5504 - acc: 0.4500 - val_loss: 1.4355 - val_acc: 0.4849\n",
      "Epoch 2/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 1.2938 - acc: 0.5471Epoch 00002: val_loss improved from 1.43546 to 1.21719, saving model to ./model_noise0_att1.h5\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 1.2933 - acc: 0.5473 - val_loss: 1.2172 - val_acc: 0.5745\n",
      "Epoch 3/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 1.1986 - acc: 0.5777Epoch 00003: val_loss improved from 1.21719 to 1.19489, saving model to ./model_noise0_att1.h5\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 1.1986 - acc: 0.5778 - val_loss: 1.1949 - val_acc: 0.5777\n",
      "Epoch 4/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 1.1374 - acc: 0.6012Epoch 00004: val_loss improved from 1.19489 to 1.10509, saving model to ./model_noise0_att1.h5\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 1.1374 - acc: 0.6012 - val_loss: 1.1051 - val_acc: 0.6084\n",
      "Epoch 5/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 1.0924 - acc: 0.6169- ETA: - ETA: 1s - lossEpoch 00005: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 1.0924 - acc: 0.6170 - val_loss: 1.2354 - val_acc: 0.5793\n",
      "Epoch 6/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 1.0573 - acc: 0.6308Epoch 00006: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 1.0571 - acc: 0.6310 - val_loss: 1.1211 - val_acc: 0.6120\n",
      "Epoch 7/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 1.0221 - acc: 0.6434Epoch 00007: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 1.0218 - acc: 0.6435 - val_loss: 1.1943 - val_acc: 0.5674\n",
      "Epoch 8/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.9945 - acc: 0.6551- ETA: 3s - l - ETA: 1s - loss: 0.995Epoch 00008: val_loss improved from 1.10509 to 1.01166, saving model to ./model_noise0_att1.h5\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.9945 - acc: 0.6551 - val_loss: 1.0117 - val_acc: 0.6454\n",
      "Epoch 9/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.9727 - acc: 0.6616Epoch 00009: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.9730 - acc: 0.6615 - val_loss: 1.1828 - val_acc: 0.5990\n",
      "Epoch 10/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.9537 - acc: 0.6685Epoch 00010: val_loss improved from 1.01166 to 0.95629, saving model to ./model_noise0_att1.h5\n",
      "782/782 [==============================] - 29s 38ms/step - loss: 0.9536 - acc: 0.6687 - val_loss: 0.9563 - val_acc: 0.6589\n",
      "Epoch 11/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.9339 - acc: 0.6759Epoch 00011: val_loss improved from 0.95629 to 0.87543, saving model to ./model_noise0_att1.h5\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.9340 - acc: 0.6759 - val_loss: 0.8754 - val_acc: 0.6937\n",
      "Epoch 12/200\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.9175 - acc: 0.6802Epoch 00012: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.9174 - acc: 0.6803 - val_loss: 0.9494 - val_acc: 0.6642\n",
      "Epoch 13/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.9012 - acc: 0.6853Epoch 00013: val_loss did not improve\n",
      "782/782 [==============================] - 29s 38ms/step - loss: 0.9011 - acc: 0.6853 - val_loss: 1.0564 - val_acc: 0.6483\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.8879 - acc: 0.6913Epoch 00014: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.8876 - acc: 0.6915 - val_loss: 0.9777 - val_acc: 0.6554\n",
      "Epoch 15/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.8695 - acc: 0.6974Epoch 00015: val_loss improved from 0.87543 to 0.84415, saving model to ./model_noise0_att1.h5\n",
      "782/782 [==============================] - 29s 38ms/step - loss: 0.8694 - acc: 0.6975 - val_loss: 0.8441 - val_acc: 0.7020\n",
      "Epoch 16/200\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.8533 - acc: 0.7032- ETA: 1s - loss: Epoch 00016: val_loss improved from 0.84415 to 0.82971, saving model to ./model_noise0_att1.h5\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 0.8536 - acc: 0.7031 - val_loss: 0.8297 - val_acc: 0.7115\n",
      "Epoch 17/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.8432 - acc: 0.7087Epoch 00017: val_loss did not improve\n",
      "782/782 [==============================] - 31s 40ms/step - loss: 0.8436 - acc: 0.7084 - val_loss: 0.9087 - val_acc: 0.6810\n",
      "Epoch 18/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.8362 - acc: 0.7086Epoch 00018: val_loss improved from 0.82971 to 0.79164, saving model to ./model_noise0_att1.h5\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.8358 - acc: 0.7088 - val_loss: 0.7916 - val_acc: 0.7260\n",
      "Epoch 19/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.8190 - acc: 0.7159-Epoch 00019: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.8190 - acc: 0.7157 - val_loss: 0.9614 - val_acc: 0.6549\n",
      "Epoch 20/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.8088 - acc: 0.7213- ETA: 2s - Epoch 00020: val_loss improved from 0.79164 to 0.78094, saving model to ./model_noise0_att1.h5\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 0.8086 - acc: 0.7213 - val_loss: 0.7809 - val_acc: 0.7302\n",
      "Epoch 21/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.8011 - acc: 0.7226Epoch 00021: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.8012 - acc: 0.7226 - val_loss: 0.7967 - val_acc: 0.7275\n",
      "Epoch 22/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.7889 - acc: 0.7253Epoch 00022: val_loss did not improve\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 0.7890 - acc: 0.7252 - val_loss: 0.8075 - val_acc: 0.7218\n",
      "Epoch 23/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.7808 - acc: 0.7286Epoch 00023: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.7809 - acc: 0.7285 - val_loss: 0.8265 - val_acc: 0.7105\n",
      "Epoch 24/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.7724 - acc: 0.7358Epoch 00024: val_loss did not improve\n",
      "782/782 [==============================] - 29s 37ms/step - loss: 0.7724 - acc: 0.7357 - val_loss: 0.8195 - val_acc: 0.7113\n",
      "Epoch 25/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.7656 - acc: 0.7363Epoch 00025: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.7651 - acc: 0.7366 - val_loss: 1.2834 - val_acc: 0.5913\n",
      "Epoch 26/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.7579 - acc: 0.7397Epoch 00026: val_loss improved from 0.78094 to 0.75717, saving model to ./model_noise0_att1.h5\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.7583 - acc: 0.7397 - val_loss: 0.7572 - val_acc: 0.7312\n",
      "Epoch 27/200\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.7462 - acc: 0.7438- ETAEpoch 00027: val_loss did not improve\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 0.7465 - acc: 0.7437 - val_loss: 0.7951 - val_acc: 0.7293\n",
      "Epoch 28/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.7396 - acc: 0.7455- ETA: 2s - lEpoch 00028: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.7394 - acc: 0.7456 - val_loss: 0.9895 - val_acc: 0.6673\n",
      "Epoch 29/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.7331 - acc: 0.7477- ETA: 0s - loss: 0.7333 - aEpoch 00029: val_loss improved from 0.75717 to 0.75622, saving model to ./model_noise0_att1.h5\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 0.7331 - acc: 0.7477 - val_loss: 0.7562 - val_acc: 0.7387\n",
      "Epoch 30/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.7246 - acc: 0.7501Epoch 00030: val_loss did not improve\n",
      "782/782 [==============================] - 29s 38ms/step - loss: 0.7243 - acc: 0.7502 - val_loss: 0.7743 - val_acc: 0.7287\n",
      "Epoch 31/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.7188 - acc: 0.7529Epoch 00031: val_loss did not improve\n",
      "782/782 [==============================] - 31s 40ms/step - loss: 0.7186 - acc: 0.7530 - val_loss: 0.7660 - val_acc: 0.7326\n",
      "Epoch 32/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.7102 - acc: 0.7557- ETA: 1s - loss: 0.710Epoch 00032: val_loss improved from 0.75622 to 0.74811, saving model to ./model_noise0_att1.h5\n",
      "782/782 [==============================] - 31s 40ms/step - loss: 0.7102 - acc: 0.7558 - val_loss: 0.7481 - val_acc: 0.7364\n",
      "Epoch 33/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.7055 - acc: 0.7595Epoch 00033: val_loss improved from 0.74811 to 0.74259, saving model to ./model_noise0_att1.h5\n",
      "782/782 [==============================] - 29s 37ms/step - loss: 0.7056 - acc: 0.7595 - val_loss: 0.7426 - val_acc: 0.7347\n",
      "Epoch 34/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.6961 - acc: 0.7621- ETA: 2s -Epoch 00034: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.6959 - acc: 0.7622 - val_loss: 0.8339 - val_acc: 0.7130\n",
      "Epoch 35/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.6950 - acc: 0.7606- ETA: 0s - loss: 0.6952 - aEpoch 00035: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.6950 - acc: 0.7606 - val_loss: 0.7644 - val_acc: 0.7331\n",
      "Epoch 36/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.6882 - acc: 0.7631Epoch 00036: val_loss improved from 0.74259 to 0.72378, saving model to ./model_noise0_att1.h5\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.6880 - acc: 0.7629 - val_loss: 0.7238 - val_acc: 0.7481\n",
      "Epoch 37/200\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.6793 - acc: 0.7680Epoch 00037: val_loss improved from 0.72378 to 0.68388, saving model to ./model_noise0_att1.h5\n",
      "782/782 [==============================] - 29s 37ms/step - loss: 0.6787 - acc: 0.7683 - val_loss: 0.6839 - val_acc: 0.7634\n",
      "Epoch 38/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.6742 - acc: 0.7684- ETA: 4s - loss: 0.67Epoch 00038: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.6743 - acc: 0.7683 - val_loss: 0.7582 - val_acc: 0.7345\n",
      "Epoch 39/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.6699 - acc: 0.7678Epoch 00039: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.6699 - acc: 0.7679 - val_loss: 0.8077 - val_acc: 0.7256\n",
      "Epoch 40/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.6617 - acc: 0.7747Epoch 00040: val_loss did not improve\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 0.6615 - acc: 0.7747 - val_loss: 0.8007 - val_acc: 0.7229\n",
      "Epoch 41/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.6572 - acc: 0.7743Epoch 00041: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.6571 - acc: 0.7744 - val_loss: 0.7298 - val_acc: 0.7485\n",
      "Epoch 42/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.6511 - acc: 0.7775Epoch 00042: val_loss improved from 0.68388 to 0.66841, saving model to ./model_noise0_att1.h5\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.6510 - acc: 0.7776 - val_loss: 0.6684 - val_acc: 0.7681\n",
      "Epoch 43/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "780/782 [============================>.] - ETA: 0s - loss: 0.6467 - acc: 0.7786- ETA: 0s - loss: 0.6466 - acc: 0.778Epoch 00043: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.6463 - acc: 0.7788 - val_loss: 0.7230 - val_acc: 0.7467\n",
      "Epoch 44/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.6422 - acc: 0.7811Epoch 00044: val_loss did not improve\n",
      "782/782 [==============================] - 29s 37ms/step - loss: 0.6419 - acc: 0.7813 - val_loss: 0.7074 - val_acc: 0.7536\n",
      "Epoch 45/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.6375 - acc: 0.7819Epoch 00045: val_loss improved from 0.66841 to 0.65181, saving model to ./model_noise0_att1.h5\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 0.6373 - acc: 0.7819 - val_loss: 0.6518 - val_acc: 0.7746\n",
      "Epoch 46/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.6335 - acc: 0.7856Epoch 00046: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.6334 - acc: 0.7856 - val_loss: 0.8295 - val_acc: 0.7007\n",
      "Epoch 47/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.6286 - acc: 0.7835Epoch 00047: val_loss did not improve\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 0.6286 - acc: 0.7835 - val_loss: 0.6662 - val_acc: 0.7729\n",
      "Epoch 48/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.6247 - acc: 0.7871Epoch 00048: val_loss improved from 0.65181 to 0.60485, saving model to ./model_noise0_att1.h5\n",
      "782/782 [==============================] - 31s 40ms/step - loss: 0.6249 - acc: 0.7870 - val_loss: 0.6049 - val_acc: 0.7925\n",
      "Epoch 49/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.6198 - acc: 0.7868- ETA: 3s - loss: 0.6206 -  - ETA: 0s - loss: 0.6195 - acc: 0.7869Epoch 00049: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.6194 - acc: 0.7870 - val_loss: 0.6484 - val_acc: 0.7731\n",
      "Epoch 50/200\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.6147 - acc: 0.7916Epoch 00050: val_loss did not improve\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 0.6147 - acc: 0.7916 - val_loss: 0.7309 - val_acc: 0.7575\n",
      "Epoch 51/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.6093 - acc: 0.7921Epoch 00051: val_loss did not improve\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 0.6096 - acc: 0.7920 - val_loss: 0.6684 - val_acc: 0.7759\n",
      "Epoch 52/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.6067 - acc: 0.7930Epoch 00052: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.6070 - acc: 0.7929 - val_loss: 0.6338 - val_acc: 0.7822\n",
      "Epoch 53/200\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.6004 - acc: 0.7938Epoch 00053: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.6008 - acc: 0.7937 - val_loss: 0.7136 - val_acc: 0.7554\n",
      "Epoch 54/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.5989 - acc: 0.7941- ETA: 1s - loss: 0.5996 - acc: 0.7 - ETA: 1s - loss: 0.5998 - ac - ETA: 0s - loss: 0.5997 - acc: 0.793 - ETA: 0s - loss: 0.5994 - ac\n",
      "Epoch 00054: reducing learning rate to 4.999999873689376e-05.\n",
      "Epoch 00054: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.5990 - acc: 0.7941 - val_loss: 0.6697 - val_acc: 0.7607\n",
      "Epoch 55/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.5770 - acc: 0.8042Epoch 00055: val_loss improved from 0.60485 to 0.58430, saving model to ./model_noise0_att1.h5\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.5770 - acc: 0.8042 - val_loss: 0.5843 - val_acc: 0.7963\n",
      "Epoch 56/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.5668 - acc: 0.8078Epoch 00056: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.5668 - acc: 0.8078 - val_loss: 0.6023 - val_acc: 0.7957\n",
      "Epoch 57/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.5650 - acc: 0.8084Epoch 00057: val_loss improved from 0.58430 to 0.55485, saving model to ./model_noise0_att1.h5\n",
      "782/782 [==============================] - 29s 38ms/step - loss: 0.5650 - acc: 0.8084 - val_loss: 0.5549 - val_acc: 0.8077\n",
      "Epoch 58/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.5653 - acc: 0.8082Epoch 00058: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.5653 - acc: 0.8082 - val_loss: 0.5686 - val_acc: 0.8033\n",
      "Epoch 59/200\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.5606 - acc: 0.8106Epoch 00059: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.5605 - acc: 0.8106 - val_loss: 0.6061 - val_acc: 0.7912\n",
      "Epoch 60/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.5600 - acc: 0.8096- ETEpoch 00060: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.5602 - acc: 0.8095 - val_loss: 0.5757 - val_acc: 0.8027\n",
      "Epoch 61/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.5551 - acc: 0.8116Epoch 00061: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.5554 - acc: 0.8115 - val_loss: 0.5598 - val_acc: 0.8094\n",
      "Epoch 62/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.5526 - acc: 0.8114Epoch 00062: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.5526 - acc: 0.8114 - val_loss: 0.6083 - val_acc: 0.7919\n",
      "Epoch 63/200\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.5572 - acc: 0.8113Epoch 00063: val_loss improved from 0.55485 to 0.55020, saving model to ./model_noise0_att1.h5\n",
      "782/782 [==============================] - 29s 37ms/step - loss: 0.5573 - acc: 0.8113 - val_loss: 0.5502 - val_acc: 0.8123\n",
      "Epoch 64/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.5512 - acc: 0.8125Epoch 00064: val_loss did not improve\n",
      "782/782 [==============================] - 29s 37ms/step - loss: 0.5512 - acc: 0.8126 - val_loss: 0.5739 - val_acc: 0.8041\n",
      "Epoch 65/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.5522 - acc: 0.8138Epoch 00065: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.5521 - acc: 0.8138 - val_loss: 0.5689 - val_acc: 0.8037\n",
      "Epoch 66/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.5441 - acc: 0.8158Epoch 00066: val_loss did not improve\n",
      "782/782 [==============================] - 29s 38ms/step - loss: 0.5441 - acc: 0.8158 - val_loss: 0.5900 - val_acc: 0.7954\n",
      "Epoch 67/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.5465 - acc: 0.8148Epoch 00067: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.5466 - acc: 0.8149 - val_loss: 0.5752 - val_acc: 0.8023\n",
      "Epoch 68/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.5415 - acc: 0.8159Epoch 00068: val_loss did not improve\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 0.5416 - acc: 0.8158 - val_loss: 0.5708 - val_acc: 0.8023\n",
      "Epoch 69/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.5400 - acc: 0.8180\n",
      "Epoch 00069: reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 00069: val_loss did not improve\n",
      "782/782 [==============================] - 29s 37ms/step - loss: 0.5400 - acc: 0.8181 - val_loss: 0.5587 - val_acc: 0.8083\n",
      "Epoch 70/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.5325 - acc: 0.8197Epoch 00070: val_loss improved from 0.55020 to 0.51553, saving model to ./model_noise0_att1.h5\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.5325 - acc: 0.8197 - val_loss: 0.5155 - val_acc: 0.8234\n",
      "Epoch 71/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.5270 - acc: 0.8209Epoch 00071: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.5269 - acc: 0.8209 - val_loss: 0.5226 - val_acc: 0.8243\n",
      "Epoch 72/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.5246 - acc: 0.8210Epoch 00072: val_loss did not improve\n",
      "782/782 [==============================] - 29s 37ms/step - loss: 0.5246 - acc: 0.8210 - val_loss: 0.5160 - val_acc: 0.8224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.5227 - acc: 0.8239Epoch 00073: val_loss did not improve\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 0.5228 - acc: 0.8239 - val_loss: 0.5171 - val_acc: 0.8262\n",
      "Epoch 74/200\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.5214 - acc: 0.8253Epoch 00074: val_loss did not improve\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 0.5213 - acc: 0.8253 - val_loss: 0.5231 - val_acc: 0.8203\n",
      "Epoch 75/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.5224 - acc: 0.8212Epoch 00075: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.5223 - acc: 0.8213 - val_loss: 0.5225 - val_acc: 0.8211\n",
      "Epoch 76/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.5217 - acc: 0.8226\n",
      "Epoch 00076: reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 00076: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.5217 - acc: 0.8226 - val_loss: 0.5404 - val_acc: 0.8122\n",
      "Epoch 77/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.5122 - acc: 0.8267Epoch 00077: val_loss improved from 0.51553 to 0.49923, saving model to ./model_noise0_att1.h5\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.5124 - acc: 0.8266 - val_loss: 0.4992 - val_acc: 0.8329\n",
      "Epoch 78/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.5145 - acc: 0.8256Epoch 00078: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.5140 - acc: 0.8260 - val_loss: 0.5000 - val_acc: 0.8316\n",
      "Epoch 79/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.5132 - acc: 0.8265 - EEpoch 00079: val_loss did not improve\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 0.5138 - acc: 0.8263 - val_loss: 0.5089 - val_acc: 0.8263\n",
      "Epoch 80/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.5175 - acc: 0.8259Epoch 00080: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.5175 - acc: 0.8259 - val_loss: 0.5014 - val_acc: 0.8273\n",
      "Epoch 81/200\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.5134 - acc: 0.8276Epoch 00081: val_loss did not improve\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 0.5136 - acc: 0.8276 - val_loss: 0.5002 - val_acc: 0.8295\n",
      "Epoch 82/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.5086 - acc: 0.8278Epoch 00082: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.5084 - acc: 0.8279 - val_loss: 0.5013 - val_acc: 0.8303\n",
      "Epoch 83/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.5101 - acc: 0.8290\n",
      "Epoch 00083: reducing learning rate to 6.24999984211172e-06.\n",
      "Epoch 00083: val_loss did not improve\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 0.5102 - acc: 0.8289 - val_loss: 0.5019 - val_acc: 0.8293\n",
      "Epoch 84/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.5075 - acc: 0.8290Epoch 00084: val_loss improved from 0.49923 to 0.49311, saving model to ./model_noise0_att1.h5\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.5075 - acc: 0.8290 - val_loss: 0.4931 - val_acc: 0.8322\n",
      "Epoch 85/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.5097 - acc: 0.8279Epoch 00085: val_loss improved from 0.49311 to 0.49130, saving model to ./model_noise0_att1.h5\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 0.5095 - acc: 0.8279 - val_loss: 0.4913 - val_acc: 0.8337\n",
      "Epoch 86/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.5074 - acc: 0.8310Epoch 00086: val_loss did not improve\n",
      "782/782 [==============================] - 31s 40ms/step - loss: 0.5073 - acc: 0.8311 - val_loss: 0.4948 - val_acc: 0.8319\n",
      "Epoch 87/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.5053 - acc: 0.8309Epoch 00087: val_loss did not improve\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 0.5052 - acc: 0.8308 - val_loss: 0.4925 - val_acc: 0.8317\n",
      "Epoch 88/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.5029 - acc: 0.8303Epoch 00088: val_loss did not improve\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 0.5028 - acc: 0.8303 - val_loss: 0.4954 - val_acc: 0.8307\n",
      "Epoch 89/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.5067 - acc: 0.8291Epoch 00089: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.5070 - acc: 0.8289 - val_loss: 0.4946 - val_acc: 0.8320\n",
      "Epoch 90/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.5049 - acc: 0.8294Epoch 00090: val_loss did not improve\n",
      "782/782 [==============================] - 29s 37ms/step - loss: 0.5049 - acc: 0.8294 - val_loss: 0.5054 - val_acc: 0.8249\n",
      "Epoch 91/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.5051 - acc: 0.8297\n",
      "Epoch 00091: reducing learning rate to 3.12499992105586e-06.\n",
      "Epoch 00091: val_loss did not improve\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 0.5050 - acc: 0.8299 - val_loss: 0.4976 - val_acc: 0.8302\n",
      "Epoch 92/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.5063 - acc: 0.8294Epoch 00092: val_loss improved from 0.49130 to 0.49041, saving model to ./model_noise0_att1.h5\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.5063 - acc: 0.8293 - val_loss: 0.4904 - val_acc: 0.8322\n",
      "Epoch 93/200\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.5020 - acc: 0.8315Epoch 00093: val_loss did not improve\n",
      "782/782 [==============================] - 29s 37ms/step - loss: 0.5017 - acc: 0.8315 - val_loss: 0.4933 - val_acc: 0.8305\n",
      "Epoch 94/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.5057 - acc: 0.8283Epoch 00094: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.5058 - acc: 0.8283 - val_loss: 0.4927 - val_acc: 0.8313\n",
      "Epoch 95/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.5018 - acc: 0.8315Epoch 00095: val_loss did not improve\n",
      "782/782 [==============================] - 31s 40ms/step - loss: 0.5016 - acc: 0.8316 - val_loss: 0.4911 - val_acc: 0.8328\n",
      "Epoch 96/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.5037 - acc: 0.8305Epoch 00096: val_loss improved from 0.49041 to 0.49031, saving model to ./model_noise0_att1.h5\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.5035 - acc: 0.8305 - val_loss: 0.4903 - val_acc: 0.8324\n",
      "Epoch 97/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.5051 - acc: 0.8306Epoch 00097: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.5056 - acc: 0.8304 - val_loss: 0.4905 - val_acc: 0.8336\n",
      "Epoch 98/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.5021 - acc: 0.8309Epoch 00098: val_loss improved from 0.49031 to 0.48976, saving model to ./model_noise0_att1.h5\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.5019 - acc: 0.8310 - val_loss: 0.4898 - val_acc: 0.8336\n",
      "Epoch 99/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.4989 - acc: 0.8332Epoch 00099: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.4992 - acc: 0.8331 - val_loss: 0.4918 - val_acc: 0.8321\n",
      "Epoch 100/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.5011 - acc: 0.8309Epoch 00100: val_loss did not improve\n",
      "782/782 [==============================] - 31s 40ms/step - loss: 0.5012 - acc: 0.8309 - val_loss: 0.4908 - val_acc: 0.8319\n",
      "Epoch 101/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.5010 - acc: 0.8307- ETA: 0s - loss: 0.5013 - acc: Epoch 00101: val_loss improved from 0.48976 to 0.48835, saving model to ./model_noise0_att1.h5\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.5008 - acc: 0.8309 - val_loss: 0.4884 - val_acc: 0.8331\n",
      "Epoch 102/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.5013 - acc: 0.8307- ETA: 2Epoch 00102: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.5015 - acc: 0.8306 - val_loss: 0.4895 - val_acc: 0.8315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 103/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.5013 - acc: 0.8327Epoch 00103: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.5014 - acc: 0.8327 - val_loss: 0.4931 - val_acc: 0.8311\n",
      "Epoch 104/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.5031 - acc: 0.8291Epoch 00104: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.5031 - acc: 0.8291 - val_loss: 0.4909 - val_acc: 0.8320\n",
      "Epoch 105/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.4990 - acc: 0.8319- ETA: 0s - loss: 0.4986 - acc:Epoch 00105: val_loss did not improve\n",
      "782/782 [==============================] - 29s 38ms/step - loss: 0.4992 - acc: 0.8317 - val_loss: 0.4901 - val_acc: 0.8332\n",
      "Epoch 106/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.5024 - acc: 0.8300Epoch 00106: val_loss improved from 0.48835 to 0.48832, saving model to ./model_noise0_att1.h5\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.5025 - acc: 0.8299 - val_loss: 0.4883 - val_acc: 0.8343\n",
      "Epoch 107/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.5000 - acc: 0.8331\n",
      "Epoch 00107: reducing learning rate to 1.56249996052793e-06.\n",
      "Epoch 00107: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.5000 - acc: 0.8331 - val_loss: 0.4886 - val_acc: 0.8336\n",
      "Epoch 108/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.4964 - acc: 0.8342Epoch 00108: val_loss improved from 0.48832 to 0.48766, saving model to ./model_noise0_att1.h5\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.4966 - acc: 0.8341 - val_loss: 0.4877 - val_acc: 0.8327\n",
      "Epoch 109/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.4986 - acc: 0.8328Epoch 00109: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.4990 - acc: 0.8327 - val_loss: 0.4918 - val_acc: 0.8324\n",
      "Epoch 110/200\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.4997 - acc: 0.8310Epoch 00110: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.4998 - acc: 0.8307 - val_loss: 0.4893 - val_acc: 0.8326\n",
      "Epoch 111/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.4999 - acc: 0.8317- ETA: Epoch 00111: val_loss improved from 0.48766 to 0.48726, saving model to ./model_noise0_att1.h5\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.4998 - acc: 0.8318 - val_loss: 0.4873 - val_acc: 0.8327\n",
      "Epoch 112/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.4980 - acc: 0.8331Epoch 00112: val_loss improved from 0.48726 to 0.48712, saving model to ./model_noise0_att1.h5\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.4977 - acc: 0.8332 - val_loss: 0.4871 - val_acc: 0.8332\n",
      "Epoch 113/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.5025 - acc: 0.8302Epoch 00113: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.5026 - acc: 0.8301 - val_loss: 0.4892 - val_acc: 0.8338\n",
      "Epoch 114/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.5037 - acc: 0.8312Epoch 00114: val_loss did not improve\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 0.5037 - acc: 0.8312 - val_loss: 0.4890 - val_acc: 0.8338\n",
      "Epoch 115/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.4981 - acc: 0.8331- ETA: 1s - loss: 0.4981Epoch 00115: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.4982 - acc: 0.8330 - val_loss: 0.4875 - val_acc: 0.8345\n",
      "Epoch 116/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.5019 - acc: 0.8302Epoch 00116: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.5020 - acc: 0.8302 - val_loss: 0.4876 - val_acc: 0.8323\n",
      "Epoch 117/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.5003 - acc: 0.8315- ETA: 2s - loss: 0.4 - ETA: 1s - loss: 0Epoch 00117: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.5002 - acc: 0.8315 - val_loss: 0.4882 - val_acc: 0.8342\n",
      "Epoch 118/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.5001 - acc: 0.8308\n",
      "Epoch 00118: reducing learning rate to 7.81249980263965e-07.\n",
      "Epoch 00118: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.4999 - acc: 0.8308 - val_loss: 0.4876 - val_acc: 0.8328\n",
      "Epoch 119/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.5001 - acc: 0.8309Epoch 00119: val_loss improved from 0.48712 to 0.48664, saving model to ./model_noise0_att1.h5\n",
      "782/782 [==============================] - 29s 37ms/step - loss: 0.4997 - acc: 0.8311 - val_loss: 0.4866 - val_acc: 0.8346\n",
      "Epoch 120/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.4980 - acc: 0.8317Epoch 00120: val_loss did not improve\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 0.4982 - acc: 0.8317 - val_loss: 0.4876 - val_acc: 0.8333\n",
      "Epoch 121/200\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.4983 - acc: 0.8323- ETA: 1s - loss: 0.4984 - acc: 0. - ETA: 1s - loss: 0.Epoch 00121: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.4983 - acc: 0.8323 - val_loss: 0.4880 - val_acc: 0.8338\n",
      "Epoch 122/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.4983 - acc: 0.8330- ETA: 0s - loss: 0.4984 - acc: 0.Epoch 00122: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.4984 - acc: 0.8329 - val_loss: 0.4884 - val_acc: 0.8334\n",
      "Epoch 123/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.5012 - acc: 0.8310Epoch 00123: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.5012 - acc: 0.8310 - val_loss: 0.4880 - val_acc: 0.8335\n",
      "Epoch 124/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.4979 - acc: 0.8331Epoch 00124: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.4981 - acc: 0.8331 - val_loss: 0.4879 - val_acc: 0.8334\n",
      "Epoch 125/200\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.4976 - acc: 0.8337- ETA: 2s - loss\n",
      "Epoch 00125: reducing learning rate to 3.906249901319825e-07.\n",
      "Epoch 00125: val_loss did not improve\n",
      "782/782 [==============================] - 31s 40ms/step - loss: 0.4980 - acc: 0.8336 - val_loss: 0.4882 - val_acc: 0.8336\n",
      "Epoch 126/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.5000 - acc: 0.8313Epoch 00126: val_loss did not improve\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 0.5000 - acc: 0.8314 - val_loss: 0.4876 - val_acc: 0.8331\n",
      "Epoch 127/200\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.5006 - acc: 0.8317Epoch 00127: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.5003 - acc: 0.8319 - val_loss: 0.4872 - val_acc: 0.8341\n",
      "Epoch 128/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.5007 - acc: 0.8312Epoch 00128: val_loss did not improve\n",
      "782/782 [==============================] - 29s 37ms/step - loss: 0.5009 - acc: 0.8311 - val_loss: 0.4875 - val_acc: 0.8336\n",
      "Epoch 129/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.4968 - acc: 0.8327Epoch 00129: val_loss improved from 0.48664 to 0.48656, saving model to ./model_noise0_att1.h5\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.4966 - acc: 0.8329 - val_loss: 0.4866 - val_acc: 0.8338\n",
      "Epoch 130/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.4989 - acc: 0.8310\n",
      "Epoch 00130: reducing learning rate to 1.9531249506599124e-07.\n",
      "Epoch 00130: val_loss did not improve\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 0.4988 - acc: 0.8311 - val_loss: 0.4869 - val_acc: 0.8350\n",
      "Epoch 131/200\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.4972 - acc: 0.8319Epoch 00131: val_loss did not improve\n",
      "782/782 [==============================] - 29s 37ms/step - loss: 0.4976 - acc: 0.8318 - val_loss: 0.4874 - val_acc: 0.8343\n",
      "Epoch 132/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "779/782 [============================>.] - ETA: 0s - loss: 0.4987 - acc: 0.8339Epoch 00132: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.4986 - acc: 0.8340 - val_loss: 0.4874 - val_acc: 0.8349\n",
      "Epoch 133/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.5009 - acc: 0.8309Epoch 00133: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.5011 - acc: 0.8308 - val_loss: 0.4869 - val_acc: 0.8344\n",
      "Epoch 134/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.4986 - acc: 0.8327Epoch 00134: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.4985 - acc: 0.8327 - val_loss: 0.4875 - val_acc: 0.8342\n",
      "Epoch 135/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.4958 - acc: 0.8329\n",
      "Epoch 00135: reducing learning rate to 9.765624753299562e-08.\n",
      "Epoch 00135: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.4962 - acc: 0.8328 - val_loss: 0.4874 - val_acc: 0.8347\n",
      "Epoch 136/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.4969 - acc: 0.8313Epoch 00136: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.4969 - acc: 0.8313 - val_loss: 0.4874 - val_acc: 0.8342\n",
      "Epoch 137/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.4997 - acc: 0.8317- ETA: 3s - loss: 0.5006 Epoch 00137: val_loss did not improve\n",
      "782/782 [==============================] - 29s 38ms/step - loss: 0.4995 - acc: 0.8318 - val_loss: 0.4873 - val_acc: 0.8340\n",
      "Epoch 138/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.4928 - acc: 0.8337- ETA: 0s - loss: 0.4927 - acc: Epoch 00138: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.4927 - acc: 0.8338 - val_loss: 0.4874 - val_acc: 0.8337\n",
      "Epoch 139/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.4993 - acc: 0.8322- ETA: 7s -Epoch 00139: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.4992 - acc: 0.8322 - val_loss: 0.4872 - val_acc: 0.8341\n",
      "An experiment done\n",
      "An experiment begin\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv_1 (Conv2D)                 (None, 32, 32, 64)   1792        input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32, 32, 64)   256         conv_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 32, 32, 64)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 16, 16, 64)   0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_2 (Conv2D)                 (None, 16, 16, 128)  73856       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 16, 16, 128)  512         conv_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 16, 16, 128)  0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 16, 16, 128)  0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 16, 16, 128)  0           activation_2[0][0]               \n",
      "                                                                 lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 8, 8, 128)    0           multiply_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 8, 8, 256)    295168      max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 8, 8, 256)    1024        conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 8, 8, 256)    0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 256)          0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10)           2570        global_average_pooling2d_1[0][0] \n",
      "==================================================================================================\n",
      "Total params: 375,178\n",
      "Trainable params: 374,282\n",
      "Non-trainable params: 896\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 1.7209 - acc: 0.3797Epoch 00001: val_loss improved from inf to 1.57549, saving model to ./model_noise1_att0.h5\n",
      "782/782 [==============================] - 33s 43ms/step - loss: 1.7204 - acc: 0.3799 - val_loss: 1.5755 - val_acc: 0.4331\n",
      "Epoch 2/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 1.5288 - acc: 0.4533Epoch 00002: val_loss improved from 1.57549 to 1.45486, saving model to ./model_noise1_att0.h5\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 1.5285 - acc: 0.4534 - val_loss: 1.4549 - val_acc: 0.4764\n",
      "Epoch 3/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 1.4612 - acc: 0.4783- ETA: 1s - loss: 1.4Epoch 00003: val_loss improved from 1.45486 to 1.41606, saving model to ./model_noise1_att0.h5\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 1.4614 - acc: 0.4782 - val_loss: 1.4161 - val_acc: 0.4907\n",
      "Epoch 4/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 1.4132 - acc: 0.4953- ETA: 2s - lEpoch 00004: val_loss improved from 1.41606 to 1.39649, saving model to ./model_noise1_att0.h5\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 1.4131 - acc: 0.4953 - val_loss: 1.3965 - val_acc: 0.4866\n",
      "Epoch 5/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 1.3766 - acc: 0.5092Epoch 00005: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 1.3764 - acc: 0.5093 - val_loss: 1.3972 - val_acc: 0.4949\n",
      "Epoch 6/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 1.3493 - acc: 0.5192- ETA: 4s - loss:  -Epoch 00006: val_loss improved from 1.39649 to 1.33253, saving model to ./model_noise1_att0.h5\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 1.3491 - acc: 0.5192 - val_loss: 1.3325 - val_acc: 0.5172\n",
      "Epoch 7/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 1.3252 - acc: 0.5300- ETA: 1s - loss: 1.3Epoch 00007: val_loss improved from 1.33253 to 1.33079, saving model to ./model_noise1_att0.h5\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 1.3259 - acc: 0.5299 - val_loss: 1.3308 - val_acc: 0.5303\n",
      "Epoch 8/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "781/782 [============================>.] - ETA: 0s - loss: 1.3048 - acc: 0.5373Epoch 00008: val_loss improved from 1.33079 to 1.25746, saving model to ./model_noise1_att0.h5\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 1.3050 - acc: 0.5372 - val_loss: 1.2575 - val_acc: 0.5520\n",
      "Epoch 9/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 1.2818 - acc: 0.5454Epoch 00009: val_loss did not improve\n",
      "782/782 [==============================] - 31s 40ms/step - loss: 1.2818 - acc: 0.5454 - val_loss: 1.3545 - val_acc: 0.5277\n",
      "Epoch 10/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 1.2662 - acc: 0.5522Epoch 00010: val_loss improved from 1.25746 to 1.22166, saving model to ./model_noise1_att0.h5\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 1.2662 - acc: 0.5522 - val_loss: 1.2217 - val_acc: 0.5570\n",
      "Epoch 11/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 1.2482 - acc: 0.5577Epoch 00011: val_loss did not improve\n",
      "782/782 [==============================] - 31s 40ms/step - loss: 1.2482 - acc: 0.5576 - val_loss: 1.2716 - val_acc: 0.5358\n",
      "Epoch 12/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 1.2322 - acc: 0.5646Epoch 00012: val_loss improved from 1.22166 to 1.20885, saving model to ./model_noise1_att0.h5\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 1.2321 - acc: 0.5647 - val_loss: 1.2088 - val_acc: 0.5678\n",
      "Epoch 13/200\n",
      "778/782 [============================>.] - ETA: 0s - loss: 1.2183 - acc: 0.5699Epoch 00013: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 1.2180 - acc: 0.5700 - val_loss: 1.2250 - val_acc: 0.5594\n",
      "Epoch 14/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 1.2034 - acc: 0.5733Epoch 00014: val_loss did not improve\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 1.2033 - acc: 0.5733 - val_loss: 1.3113 - val_acc: 0.5338\n",
      "Epoch 15/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 1.1925 - acc: 0.5813Epoch 00015: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 1.1925 - acc: 0.5812 - val_loss: 1.2239 - val_acc: 0.5613\n",
      "Epoch 16/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 1.1804 - acc: 0.5828Epoch 00016: val_loss improved from 1.20885 to 1.17403, saving model to ./model_noise1_att0.h5\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 1.1803 - acc: 0.5828 - val_loss: 1.1740 - val_acc: 0.5785\n",
      "Epoch 17/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 1.1657 - acc: 0.5887- ETA: 2s - loss: 1.1661 - acc:  - ETA: 2sEpoch 00017: val_loss improved from 1.17403 to 1.14839, saving model to ./model_noise1_att0.h5\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 1.1654 - acc: 0.5889 - val_loss: 1.1484 - val_acc: 0.5857\n",
      "Epoch 18/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 1.1589 - acc: 0.5922Epoch 00018: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 1.1589 - acc: 0.5922 - val_loss: 1.1704 - val_acc: 0.5862\n",
      "Epoch 19/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 1.1498 - acc: 0.5948- ETA: 0s - loss: 1.1503 - acc:Epoch 00019: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 1.1501 - acc: 0.5947 - val_loss: 1.1858 - val_acc: 0.5753\n",
      "Epoch 20/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 1.1369 - acc: 0.6002Epoch 00020: val_loss improved from 1.14839 to 1.09360, saving model to ./model_noise1_att0.h5\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 1.1367 - acc: 0.6003 - val_loss: 1.0936 - val_acc: 0.6073\n",
      "Epoch 21/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 1.1292 - acc: 0.6025Epoch 00021: val_loss did not improve\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 1.1293 - acc: 0.6025 - val_loss: 1.1196 - val_acc: 0.6017\n",
      "Epoch 22/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 1.1177 - acc: 0.6050Epoch 00022: val_loss did not improve\n",
      "782/782 [==============================] - 31s 40ms/step - loss: 1.1177 - acc: 0.6051 - val_loss: 1.1535 - val_acc: 0.5894\n",
      "Epoch 23/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 1.1057 - acc: 0.6120Epoch 00023: val_loss did not improve\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 1.1057 - acc: 0.6119 - val_loss: 1.2428 - val_acc: 0.5535\n",
      "Epoch 24/200\n",
      "778/782 [============================>.] - ETA: 0s - loss: 1.1014 - acc: 0.6128Epoch 00024: val_loss did not improve\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 1.1014 - acc: 0.6128 - val_loss: 1.1299 - val_acc: 0.5888\n",
      "Epoch 25/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 1.0921 - acc: 0.6172Epoch 00025: val_loss did not improve\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 1.0922 - acc: 0.6172 - val_loss: 1.1243 - val_acc: 0.6051\n",
      "Epoch 26/200\n",
      "778/782 [============================>.] - ETA: 0s - loss: 1.0882 - acc: 0.6197\n",
      "Epoch 00026: reducing learning rate to 4.999999873689376e-05.\n",
      "Epoch 00026: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 1.0879 - acc: 0.6200 - val_loss: 1.1154 - val_acc: 0.6006\n",
      "Epoch 27/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 1.0640 - acc: 0.6287- ETA: 2s Epoch 00027: val_loss improved from 1.09360 to 1.03582, saving model to ./model_noise1_att0.h5\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 1.0641 - acc: 0.6288 - val_loss: 1.0358 - val_acc: 0.6296\n",
      "Epoch 28/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 1.0538 - acc: 0.6336Epoch 00028: val_loss improved from 1.03582 to 1.01846, saving model to ./model_noise1_att0.h5\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 1.0541 - acc: 0.6335 - val_loss: 1.0185 - val_acc: 0.6379\n",
      "Epoch 29/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 1.0524 - acc: 0.6334- ETA: 0s - loss: 1.0523 - acc: 0.633Epoch 00029: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 1.0524 - acc: 0.6335 - val_loss: 1.0837 - val_acc: 0.6138\n",
      "Epoch 30/200\n",
      "778/782 [============================>.] - ETA: 0s - loss: 1.0498 - acc: 0.6345- ETA: 1s - loss: 1.0488 Epoch 00030: val_loss improved from 1.01846 to 1.00542, saving model to ./model_noise1_att0.h5\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 1.0499 - acc: 0.6345 - val_loss: 1.0054 - val_acc: 0.6463\n",
      "Epoch 31/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 1.0462 - acc: 0.6362Epoch 00031: val_loss did not improve\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 1.0464 - acc: 0.6361 - val_loss: 1.0597 - val_acc: 0.6305\n",
      "Epoch 32/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 1.0369 - acc: 0.6379Epoch 00032: val_loss improved from 1.00542 to 1.00383, saving model to ./model_noise1_att0.h5\n",
      "782/782 [==============================] - 29s 37ms/step - loss: 1.0369 - acc: 0.6378 - val_loss: 1.0038 - val_acc: 0.6473\n",
      "Epoch 33/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 1.0363 - acc: 0.6391Epoch 00033: val_loss improved from 1.00383 to 0.99897, saving model to ./model_noise1_att0.h5\n",
      "782/782 [==============================] - 29s 37ms/step - loss: 1.0360 - acc: 0.6390 - val_loss: 0.9990 - val_acc: 0.6485\n",
      "Epoch 34/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 1.0313 - acc: 0.6400Epoch 00034: val_loss did not improve\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 1.0314 - acc: 0.6399 - val_loss: 1.0293 - val_acc: 0.6356\n",
      "Epoch 35/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 1.0275 - acc: 0.6426Epoch 00035: val_loss did not improve\n",
      "782/782 [==============================] - 31s 40ms/step - loss: 1.0272 - acc: 0.6427 - val_loss: 1.0217 - val_acc: 0.6390\n",
      "Epoch 36/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 1.0249 - acc: 0.6418Epoch 00036: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 1.0251 - acc: 0.6417 - val_loss: 1.0422 - val_acc: 0.6327\n",
      "Epoch 37/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "780/782 [============================>.] - ETA: 0s - loss: 1.0198 - acc: 0.6455Epoch 00037: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 1.0199 - acc: 0.6456 - val_loss: 1.0522 - val_acc: 0.6227\n",
      "Epoch 38/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 1.0168 - acc: 0.6462Epoch 00038: val_loss did not improve\n",
      "782/782 [==============================] - 29s 38ms/step - loss: 1.0167 - acc: 0.6463 - val_loss: 1.0335 - val_acc: 0.6326\n",
      "Epoch 39/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 1.0152 - acc: 0.6471Epoch 00039: val_loss improved from 0.99897 to 0.98994, saving model to ./model_noise1_att0.h5\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 1.0150 - acc: 0.6472 - val_loss: 0.9899 - val_acc: 0.6501\n",
      "Epoch 40/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 1.0087 - acc: 0.6484Epoch 00040: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 1.0092 - acc: 0.6483 - val_loss: 1.0185 - val_acc: 0.6401\n",
      "Epoch 41/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 1.0042 - acc: 0.6513Epoch 00041: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 1.0045 - acc: 0.6513 - val_loss: 1.0099 - val_acc: 0.6425\n",
      "Epoch 42/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 1.0010 - acc: 0.6519Epoch 00042: val_loss improved from 0.98994 to 0.98431, saving model to ./model_noise1_att0.h5\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 1.0012 - acc: 0.6518 - val_loss: 0.9843 - val_acc: 0.6508\n",
      "Epoch 43/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 1.0015 - acc: 0.6520Epoch 00043: val_loss improved from 0.98431 to 0.98120, saving model to ./model_noise1_att0.h5\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 1.0013 - acc: 0.6520 - val_loss: 0.9812 - val_acc: 0.6524\n",
      "Epoch 44/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.9976 - acc: 0.6545- ETA: 0s - loss: 0.9969 - acc:Epoch 00044: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.9974 - acc: 0.6547 - val_loss: 1.0201 - val_acc: 0.6397\n",
      "Epoch 45/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.9939 - acc: 0.6571Epoch 00045: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.9937 - acc: 0.6572 - val_loss: 1.0144 - val_acc: 0.6379\n",
      "Epoch 46/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.9904 - acc: 0.6568-Epoch 00046: val_loss did not improve\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 0.9906 - acc: 0.6567 - val_loss: 1.0000 - val_acc: 0.6467\n",
      "Epoch 47/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.9848 - acc: 0.6584- E - ETA: 0s - loss: 0.9837 - Epoch 00047: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.9849 - acc: 0.6583 - val_loss: 1.0094 - val_acc: 0.6408\n",
      "Epoch 48/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.9851 - acc: 0.6560Epoch 00048: val_loss improved from 0.98120 to 0.96800, saving model to ./model_noise1_att0.h5\n",
      "782/782 [==============================] - 29s 37ms/step - loss: 0.9850 - acc: 0.6562 - val_loss: 0.9680 - val_acc: 0.6636\n",
      "Epoch 49/200\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.9840 - acc: 0.6574Epoch 00049: val_loss did not improve\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 0.9840 - acc: 0.6574 - val_loss: 0.9986 - val_acc: 0.6433\n",
      "Epoch 50/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.9766 - acc: 0.6621Epoch 00050: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.9768 - acc: 0.6621 - val_loss: 0.9986 - val_acc: 0.6403\n",
      "Epoch 51/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.9751 - acc: 0.6605Epoch 00051: val_loss improved from 0.96800 to 0.96499, saving model to ./model_noise1_att0.h5\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.9749 - acc: 0.6606 - val_loss: 0.9650 - val_acc: 0.6558\n",
      "Epoch 52/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.9735 - acc: 0.6603Epoch 00052: val_loss improved from 0.96499 to 0.96357, saving model to ./model_noise1_att0.h5\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 0.9739 - acc: 0.6603 - val_loss: 0.9636 - val_acc: 0.6572\n",
      "Epoch 53/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.9724 - acc: 0.6632Epoch 00053: val_loss did not improve\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 0.9723 - acc: 0.6631 - val_loss: 0.9915 - val_acc: 0.6480\n",
      "Epoch 54/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.9673 - acc: 0.6657Epoch 00054: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.9672 - acc: 0.6657 - val_loss: 0.9809 - val_acc: 0.6575\n",
      "Epoch 55/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.9636 - acc: 0.6646Epoch 00055: val_loss did not improve\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 0.9635 - acc: 0.6646 - val_loss: 1.0150 - val_acc: 0.6385\n",
      "Epoch 56/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.9641 - acc: 0.6650Epoch 00056: val_loss did not improve\n",
      "782/782 [==============================] - 29s 38ms/step - loss: 0.9642 - acc: 0.6651 - val_loss: 0.9681 - val_acc: 0.6586\n",
      "Epoch 57/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.9608 - acc: 0.6668  ETA: 10s -Epoch 00057: val_loss did not improve\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 0.9608 - acc: 0.6668 - val_loss: 0.9811 - val_acc: 0.6532\n",
      "Epoch 58/200\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.9517 - acc: 0.6695- ETA: 4s - loEpoch 00058: val_loss improved from 0.96357 to 0.95954, saving model to ./model_noise1_att0.h5\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.9520 - acc: 0.6696 - val_loss: 0.9595 - val_acc: 0.6622\n",
      "Epoch 59/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.9566 - acc: 0.6679Epoch 00059: val_loss improved from 0.95954 to 0.94757, saving model to ./model_noise1_att0.h5\n",
      "782/782 [==============================] - 31s 40ms/step - loss: 0.9565 - acc: 0.6680 - val_loss: 0.9476 - val_acc: 0.6685\n",
      "Epoch 60/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.9521 - acc: 0.6690- ETA: 1s - loss: 0.9527 -Epoch 00060: val_loss improved from 0.94757 to 0.92682, saving model to ./model_noise1_att0.h5\n",
      "782/782 [==============================] - 32s 40ms/step - loss: 0.9526 - acc: 0.6689 - val_loss: 0.9268 - val_acc: 0.6707\n",
      "Epoch 61/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.9514 - acc: 0.6695- ETA: 2s - lEpoch 00061: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.9517 - acc: 0.6694 - val_loss: 0.9497 - val_acc: 0.6675\n",
      "Epoch 62/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.9474 - acc: 0.6726Epoch 00062: val_loss did not improve\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 0.9471 - acc: 0.6727 - val_loss: 1.0224 - val_acc: 0.6386\n",
      "Epoch 63/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.9440 - acc: 0.6750Epoch 00063: val_loss did not improve\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 0.9441 - acc: 0.6749 - val_loss: 0.9794 - val_acc: 0.6557\n",
      "Epoch 64/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.9423 - acc: 0.6753Epoch 00064: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.9423 - acc: 0.6753 - val_loss: 0.9387 - val_acc: 0.6712\n",
      "Epoch 65/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.9392 - acc: 0.6745Epoch 00065: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.9392 - acc: 0.6745 - val_loss: 0.9565 - val_acc: 0.6614\n",
      "Epoch 66/200\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.9387 - acc: 0.6752\n",
      "Epoch 00066: reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 00066: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.9389 - acc: 0.6750 - val_loss: 0.9515 - val_acc: 0.6568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.9250 - acc: 0.6818Epoch 00067: val_loss improved from 0.92682 to 0.90540, saving model to ./model_noise1_att0.h5\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.9250 - acc: 0.6818 - val_loss: 0.9054 - val_acc: 0.6818\n",
      "Epoch 68/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.9211 - acc: 0.6806- EEpoch 00068: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.9214 - acc: 0.6804 - val_loss: 0.9154 - val_acc: 0.6773\n",
      "Epoch 69/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.9208 - acc: 0.6812Epoch 00069: val_loss did not improve\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 0.9205 - acc: 0.6813 - val_loss: 0.9071 - val_acc: 0.6808\n",
      "Epoch 70/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.9188 - acc: 0.6813Epoch 00070: val_loss improved from 0.90540 to 0.90313, saving model to ./model_noise1_att0.h5\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.9189 - acc: 0.6813 - val_loss: 0.9031 - val_acc: 0.6845\n",
      "Epoch 71/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.9176 - acc: 0.6840- ETA:Epoch 00071: val_loss improved from 0.90313 to 0.90310, saving model to ./model_noise1_att0.h5\n",
      "782/782 [==============================] - 31s 40ms/step - loss: 0.9181 - acc: 0.6838 - val_loss: 0.9031 - val_acc: 0.6856\n",
      "Epoch 72/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.9161 - acc: 0.6829- ETA: 1s - loss: 0 - ETA: 0s - loss: 0.9165 - acc: 0.6Epoch 00072: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.9163 - acc: 0.6828 - val_loss: 0.9424 - val_acc: 0.6656\n",
      "Epoch 73/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.9144 - acc: 0.6825Epoch 00073: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.9143 - acc: 0.6825 - val_loss: 0.9097 - val_acc: 0.6823\n",
      "Epoch 74/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.9127 - acc: 0.6857Epoch 00074: val_loss did not improve\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 0.9126 - acc: 0.6857 - val_loss: 0.9051 - val_acc: 0.6784\n",
      "Epoch 75/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.9112 - acc: 0.6852Epoch 00075: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.9113 - acc: 0.6851 - val_loss: 0.9098 - val_acc: 0.6805\n",
      "Epoch 76/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.9118 - acc: 0.6860- ETA - ETA: 1s \n",
      "Epoch 00076: reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 00076: val_loss did not improve\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 0.9116 - acc: 0.6860 - val_loss: 0.9175 - val_acc: 0.6751\n",
      "Epoch 77/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.9020 - acc: 0.6892Epoch 00077: val_loss improved from 0.90310 to 0.89417, saving model to ./model_noise1_att0.h5\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 0.9021 - acc: 0.6893 - val_loss: 0.8942 - val_acc: 0.6872\n",
      "Epoch 78/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.8997 - acc: 0.6909Epoch 00078: val_loss improved from 0.89417 to 0.89294, saving model to ./model_noise1_att0.h5\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.8997 - acc: 0.6909 - val_loss: 0.8929 - val_acc: 0.6866\n",
      "Epoch 79/200\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.9052 - acc: 0.6891Epoch 00079: val_loss improved from 0.89294 to 0.89028, saving model to ./model_noise1_att0.h5\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.9052 - acc: 0.6891 - val_loss: 0.8903 - val_acc: 0.6917\n",
      "Epoch 80/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.9000 - acc: 0.690 - ETA: 0s - loss: 0.9000 - acc: 0.6906Epoch 00080: val_loss improved from 0.89028 to 0.88968, saving model to ./model_noise1_att0.h5\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 0.8998 - acc: 0.6906 - val_loss: 0.8897 - val_acc: 0.6875\n",
      "Epoch 81/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.8993 - acc: 0.6900Epoch 00081: val_loss improved from 0.88968 to 0.88464, saving model to ./model_noise1_att0.h5\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 0.8992 - acc: 0.6901 - val_loss: 0.8846 - val_acc: 0.6905\n",
      "Epoch 82/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.9013 - acc: 0.6899Epoch 00082: val_loss did not improve\n",
      "782/782 [==============================] - 31s 40ms/step - loss: 0.9011 - acc: 0.6901 - val_loss: 0.9059 - val_acc: 0.6823\n",
      "Epoch 83/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.8982 - acc: 0.6919Epoch 00083: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.8985 - acc: 0.6919 - val_loss: 0.8923 - val_acc: 0.6871\n",
      "Epoch 84/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.8991 - acc: 0.6913Epoch 00084: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.8991 - acc: 0.6913 - val_loss: 0.8990 - val_acc: 0.6836\n",
      "Epoch 85/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.8994 - acc: 0.6901- ETA:Epoch 00085: val_loss did not improve\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 0.8996 - acc: 0.6902 - val_loss: 0.8858 - val_acc: 0.6883\n",
      "Epoch 86/200\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.8971 - acc: 0.6913Epoch 00086: val_loss improved from 0.88464 to 0.88230, saving model to ./model_noise1_att0.h5\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.8977 - acc: 0.6910 - val_loss: 0.8823 - val_acc: 0.6914\n",
      "Epoch 87/200\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.8964 - acc: 0.689 - ETA: 0s - loss: 0.8964 - acc: 0.6890Epoch 00087: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.8965 - acc: 0.6889 - val_loss: 0.8874 - val_acc: 0.6878\n",
      "Epoch 88/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.8920 - acc: 0.6932Epoch 00088: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.8922 - acc: 0.6930 - val_loss: 0.8829 - val_acc: 0.6891\n",
      "Epoch 89/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.8945 - acc: 0.6922Epoch 00089: val_loss improved from 0.88230 to 0.87668, saving model to ./model_noise1_att0.h5\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.8944 - acc: 0.6921 - val_loss: 0.8767 - val_acc: 0.6925\n",
      "Epoch 90/200\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.8949 - acc: 0.6929Epoch 00090: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.8953 - acc: 0.6928 - val_loss: 0.8854 - val_acc: 0.6895\n",
      "Epoch 91/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.8953 - acc: 0.6920- ETA: 0s - loss: 0.8956 - Epoch 00091: val_loss did not improve\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 0.8956 - acc: 0.6919 - val_loss: 0.8818 - val_acc: 0.6889\n",
      "Epoch 92/200\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.8930 - acc: 0.6896Epoch 00092: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.8932 - acc: 0.6896 - val_loss: 0.8773 - val_acc: 0.6892\n",
      "Epoch 93/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.8935 - acc: 0.6923- E - ETA: 1s - lossEpoch 00093: val_loss did not improve\n",
      "782/782 [==============================] - 29s 37ms/step - loss: 0.8934 - acc: 0.6923 - val_loss: 0.8964 - val_acc: 0.6865\n",
      "Epoch 94/200\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.8921 - acc: 0.6934Epoch 00094: val_loss did not improve\n",
      "782/782 [==============================] - 29s 38ms/step - loss: 0.8922 - acc: 0.6932 - val_loss: 0.8785 - val_acc: 0.6924\n",
      "Epoch 95/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.8953 - acc: 0.6909- ETA: 1s - loss: 0.8957 \n",
      "Epoch 00095: reducing learning rate to 6.24999984211172e-06.\n",
      "Epoch 00095: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.8952 - acc: 0.6910 - val_loss: 0.8882 - val_acc: 0.6872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.8871 - acc: 0.6952Epoch 00096: val_loss improved from 0.87668 to 0.87356, saving model to ./model_noise1_att0.h5\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.8868 - acc: 0.6952 - val_loss: 0.8736 - val_acc: 0.6924\n",
      "Epoch 97/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.8879 - acc: 0.6935Epoch 00097: val_loss improved from 0.87356 to 0.87031, saving model to ./model_noise1_att0.h5\n",
      "782/782 [==============================] - 29s 37ms/step - loss: 0.8881 - acc: 0.6935 - val_loss: 0.8703 - val_acc: 0.6953\n",
      "Epoch 98/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.8895 - acc: 0.6932Epoch 00098: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.8894 - acc: 0.6934 - val_loss: 0.8742 - val_acc: 0.6957\n",
      "Epoch 99/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.8861 - acc: 0.6946Epoch 00099: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.8860 - acc: 0.6947 - val_loss: 0.8718 - val_acc: 0.6931\n",
      "Epoch 100/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.8850 - acc: 0.6963Epoch 00100: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.8851 - acc: 0.6964 - val_loss: 0.8714 - val_acc: 0.6945\n",
      "Epoch 101/200\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.8869 - acc: 0.6944- ETA: 0s - loss: 0.8874 - acc: 0.6Epoch 00101: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.8872 - acc: 0.6944 - val_loss: 0.8766 - val_acc: 0.6898\n",
      "Epoch 102/200\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.8846 - acc: 0.6962Epoch 00102: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.8845 - acc: 0.6961 - val_loss: 0.8738 - val_acc: 0.6942\n",
      "Epoch 103/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.8824 - acc: 0.6963Epoch 00103: val_loss improved from 0.87031 to 0.86941, saving model to ./model_noise1_att0.h5\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.8827 - acc: 0.6960 - val_loss: 0.8694 - val_acc: 0.6946\n",
      "Epoch 104/200\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.8834 - acc: 0.6926Epoch 00104: val_loss improved from 0.86941 to 0.86880, saving model to ./model_noise1_att0.h5\n",
      "782/782 [==============================] - 31s 40ms/step - loss: 0.8836 - acc: 0.6926 - val_loss: 0.8688 - val_acc: 0.6974\n",
      "Epoch 105/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.8836 - acc: 0.6963- ETA: 4s - loss: 0.8837 - acEpoch 00105: val_loss did not improve\n",
      "782/782 [==============================] - 31s 40ms/step - loss: 0.8834 - acc: 0.6965 - val_loss: 0.8691 - val_acc: 0.6955\n",
      "Epoch 106/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.8829 - acc: 0.6966Epoch 00106: val_loss improved from 0.86880 to 0.86865, saving model to ./model_noise1_att0.h5\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 0.8832 - acc: 0.6964 - val_loss: 0.8687 - val_acc: 0.6938\n",
      "Epoch 107/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.8874 - acc: 0.6966Epoch 00107: val_loss did not improve\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 0.8873 - acc: 0.6966 - val_loss: 0.8744 - val_acc: 0.6933\n",
      "Epoch 108/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.8846 - acc: 0.6948Epoch 00108: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.8839 - acc: 0.6950 - val_loss: 0.8699 - val_acc: 0.6926\n",
      "Epoch 109/200\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.8828 - acc: 0.6958Epoch 00109: val_loss improved from 0.86865 to 0.86802, saving model to ./model_noise1_att0.h5\n",
      "782/782 [==============================] - 31s 40ms/step - loss: 0.8828 - acc: 0.6957 - val_loss: 0.8680 - val_acc: 0.6952\n",
      "Epoch 110/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.8842 - acc: 0.6957Epoch 00110: val_loss improved from 0.86802 to 0.86727, saving model to ./model_noise1_att0.h5\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.8841 - acc: 0.6958 - val_loss: 0.8673 - val_acc: 0.6953\n",
      "Epoch 111/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.8839 - acc: 0.6945Epoch 00111: val_loss did not improve\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 0.8845 - acc: 0.6943 - val_loss: 0.8690 - val_acc: 0.6949\n",
      "Epoch 112/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.8798 - acc: 0.6970Epoch 00112: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.8796 - acc: 0.6972 - val_loss: 0.8683 - val_acc: 0.6948\n",
      "Epoch 113/200\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.8818 - acc: 0.6953Epoch 00113: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.8815 - acc: 0.6955 - val_loss: 0.8700 - val_acc: 0.6937\n",
      "Epoch 114/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.8779 - acc: 0.6970Epoch 00114: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.8776 - acc: 0.6970 - val_loss: 0.8685 - val_acc: 0.6937\n",
      "Epoch 115/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.8790 - acc: 0.6971Epoch 00115: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.8791 - acc: 0.6970 - val_loss: 0.8739 - val_acc: 0.6931\n",
      "Epoch 116/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.8788 - acc: 0.6967\n",
      "Epoch 00116: reducing learning rate to 3.12499992105586e-06.\n",
      "Epoch 00116: val_loss did not improve\n",
      "782/782 [==============================] - 31s 40ms/step - loss: 0.8787 - acc: 0.6968 - val_loss: 0.8712 - val_acc: 0.6934\n",
      "Epoch 117/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.8779 - acc: 0.6966- ETA: 7s - lo - ETEpoch 00117: val_loss improved from 0.86727 to 0.86587, saving model to ./model_noise1_att0.h5\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.8778 - acc: 0.6966 - val_loss: 0.8659 - val_acc: 0.6939\n",
      "Epoch 118/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.8757 - acc: 0.6979Epoch 00118: val_loss did not improve\n",
      "782/782 [==============================] - 29s 37ms/step - loss: 0.8758 - acc: 0.6977 - val_loss: 0.8668 - val_acc: 0.6953\n",
      "Epoch 119/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.8762 - acc: 0.6999- ETA: 2s - losEpoch 00119: val_loss improved from 0.86587 to 0.86480, saving model to ./model_noise1_att0.h5\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.8760 - acc: 0.6999 - val_loss: 0.8648 - val_acc: 0.6974\n",
      "Epoch 120/200\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.8750 - acc: 0.6994- ETA: 2sEpoch 00120: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.8751 - acc: 0.6995 - val_loss: 0.8660 - val_acc: 0.6960\n",
      "Epoch 121/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.8787 - acc: 0.6961- ETA: 3s - loss: 0.881 - ETA: 2Epoch 00121: val_loss improved from 0.86480 to 0.86378, saving model to ./model_noise1_att0.h5\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.8789 - acc: 0.6961 - val_loss: 0.8638 - val_acc: 0.6963\n",
      "Epoch 122/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.8750 - acc: 0.7000Epoch 00122: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.8756 - acc: 0.6998 - val_loss: 0.8641 - val_acc: 0.6973\n",
      "Epoch 123/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.8760 - acc: 0.6984Epoch 00123: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.8758 - acc: 0.6985 - val_loss: 0.8682 - val_acc: 0.6947\n",
      "Epoch 124/200\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.8767 - acc: 0.6982- ETA: 1s - loss:Epoch 00124: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.8767 - acc: 0.6982 - val_loss: 0.8644 - val_acc: 0.6959\n",
      "Epoch 125/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "778/782 [============================>.] - ETA: 0s - loss: 0.8770 - acc: 0.6979Epoch 00125: val_loss improved from 0.86378 to 0.86360, saving model to ./model_noise1_att0.h5\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.8771 - acc: 0.6979 - val_loss: 0.8636 - val_acc: 0.6958\n",
      "Epoch 126/200\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.8786 - acc: 0.6981Epoch 00126: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.8784 - acc: 0.6982 - val_loss: 0.8665 - val_acc: 0.6947\n",
      "Epoch 127/200\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.8761 - acc: 0.6985Epoch 00127: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.8769 - acc: 0.6982 - val_loss: 0.8640 - val_acc: 0.6967\n",
      "Epoch 128/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.8747 - acc: 0.6983- ETA: Epoch 00128: val_loss improved from 0.86360 to 0.86307, saving model to ./model_noise1_att0.h5\n",
      "782/782 [==============================] - 31s 40ms/step - loss: 0.8745 - acc: 0.6984 - val_loss: 0.8631 - val_acc: 0.6967\n",
      "Epoch 129/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.8741 - acc: 0.6986Epoch 00129: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.8741 - acc: 0.6985 - val_loss: 0.8650 - val_acc: 0.6973\n",
      "Epoch 130/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.8751 - acc: 0.6993Epoch 00130: val_loss improved from 0.86307 to 0.86212, saving model to ./model_noise1_att0.h5\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.8753 - acc: 0.6992 - val_loss: 0.8621 - val_acc: 0.6985\n",
      "Epoch 131/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.8754 - acc: 0.6989Epoch 00131: val_loss did not improve\n",
      "782/782 [==============================] - 29s 38ms/step - loss: 0.8752 - acc: 0.6989 - val_loss: 0.8629 - val_acc: 0.6975\n",
      "Epoch 132/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.8743 - acc: 0.7005Epoch 00132: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.8746 - acc: 0.7005 - val_loss: 0.8640 - val_acc: 0.6957\n",
      "Epoch 133/200\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.8759 - acc: 0.6993Epoch 00133: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.8763 - acc: 0.6992 - val_loss: 0.8650 - val_acc: 0.6963\n",
      "Epoch 134/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.8737 - acc: 0.6995Epoch 00134: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.8739 - acc: 0.6993 - val_loss: 0.8633 - val_acc: 0.6979\n",
      "Epoch 135/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.8751 - acc: 0.6988Epoch 00135: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.8752 - acc: 0.6986 - val_loss: 0.8628 - val_acc: 0.6975\n",
      "Epoch 136/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.8771 - acc: 0.6975\n",
      "Epoch 00136: reducing learning rate to 1.56249996052793e-06.\n",
      "Epoch 00136: val_loss did not improve\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 0.8772 - acc: 0.6975 - val_loss: 0.8626 - val_acc: 0.6996\n",
      "Epoch 137/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.8741 - acc: 0.7007Epoch 00137: val_loss improved from 0.86212 to 0.86113, saving model to ./model_noise1_att0.h5\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.8744 - acc: 0.7005 - val_loss: 0.8611 - val_acc: 0.6960\n",
      "Epoch 138/200\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.8761 - acc: 0.6973- ETA: 0s - loss: 0.8758 - acc: 0.69Epoch 00138: val_loss did not improve\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 0.8759 - acc: 0.6976 - val_loss: 0.8623 - val_acc: 0.6967\n",
      "Epoch 139/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.8743 - acc: 0.7005Epoch 00139: val_loss improved from 0.86113 to 0.86107, saving model to ./model_noise1_att0.h5\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 0.8744 - acc: 0.7006 - val_loss: 0.8611 - val_acc: 0.6993\n",
      "Epoch 140/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.8739 - acc: 0.7011Epoch 00140: val_loss did not improve\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 0.8741 - acc: 0.7010 - val_loss: 0.8626 - val_acc: 0.6969\n",
      "Epoch 141/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.8747 - acc: 0.6979Epoch 00141: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.8748 - acc: 0.6979 - val_loss: 0.8617 - val_acc: 0.6982\n",
      "Epoch 142/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.8757 - acc: 0.6997- ETEpoch 00142: val_loss did not improve\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 0.8759 - acc: 0.6997 - val_loss: 0.8617 - val_acc: 0.6966\n",
      "Epoch 143/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.8725 - acc: 0.7009\n",
      "Epoch 00143: reducing learning rate to 7.81249980263965e-07.\n",
      "Epoch 00143: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.8725 - acc: 0.7009 - val_loss: 0.8626 - val_acc: 0.6974\n",
      "Epoch 144/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.8743 - acc: 0.6998Epoch 00144: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.8742 - acc: 0.6998 - val_loss: 0.8614 - val_acc: 0.6972\n",
      "Epoch 145/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.8734 - acc: 0.6996Epoch 00145: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.8732 - acc: 0.6997 - val_loss: 0.8616 - val_acc: 0.6964\n",
      "Epoch 146/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.8735 - acc: 0.6989Epoch 00146: val_loss did not improve\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 0.8735 - acc: 0.6989 - val_loss: 0.8625 - val_acc: 0.6963\n",
      "Epoch 147/200\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.8689 - acc: 0.7050Epoch 00147: val_loss did not improve\n",
      "782/782 [==============================] - 31s 40ms/step - loss: 0.8690 - acc: 0.7050 - val_loss: 0.8619 - val_acc: 0.6975\n",
      "Epoch 148/200\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.8688 - acc: 0.7022\n",
      "Epoch 00148: reducing learning rate to 3.906249901319825e-07.\n",
      "Epoch 00148: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.8685 - acc: 0.7025 - val_loss: 0.8617 - val_acc: 0.6953\n",
      "Epoch 149/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.8721 - acc: 0.7003Epoch 00149: val_loss improved from 0.86107 to 0.86065, saving model to ./model_noise1_att0.h5\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.8725 - acc: 0.7003 - val_loss: 0.8607 - val_acc: 0.6975\n",
      "Epoch 150/200\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.8726 - acc: 0.7017Epoch 00150: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.8725 - acc: 0.7017 - val_loss: 0.8615 - val_acc: 0.6963\n",
      "Epoch 151/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.8711 - acc: 0.7011Epoch 00151: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.8709 - acc: 0.7012 - val_loss: 0.8613 - val_acc: 0.6971\n",
      "Epoch 152/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.8717 - acc: 0.7002Epoch 00152: val_loss did not improve\n",
      "782/782 [==============================] - 29s 38ms/step - loss: 0.8715 - acc: 0.7002 - val_loss: 0.8612 - val_acc: 0.6967\n",
      "Epoch 153/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.8707 - acc: 0.7002Epoch 00153: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.8703 - acc: 0.7005 - val_loss: 0.8610 - val_acc: 0.6976\n",
      "Epoch 154/200\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.8702 - acc: 0.7005Epoch 00154: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.8702 - acc: 0.7007 - val_loss: 0.8614 - val_acc: 0.6960\n",
      "Epoch 155/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "779/782 [============================>.] - ETA: 0s - loss: 0.8757 - acc: 0.6997\n",
      "Epoch 00155: reducing learning rate to 1.9531249506599124e-07.\n",
      "Epoch 00155: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.8757 - acc: 0.6996 - val_loss: 0.8615 - val_acc: 0.6968\n",
      "Epoch 156/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.8730 - acc: 0.6995- ETA: 1s - loss: 0.8Epoch 00156: val_loss did not improve\n",
      "782/782 [==============================] - 31s 40ms/step - loss: 0.8729 - acc: 0.6996 - val_loss: 0.8617 - val_acc: 0.6971\n",
      "Epoch 157/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.8741 - acc: 0.6986Epoch 00157: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.8740 - acc: 0.6986 - val_loss: 0.8617 - val_acc: 0.6969\n",
      "Epoch 158/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.8722 - acc: 0.6994Epoch 00158: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.8722 - acc: 0.6995 - val_loss: 0.8612 - val_acc: 0.6973\n",
      "Epoch 159/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.8755 - acc: 0.6993Epoch 00159: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.8756 - acc: 0.6992 - val_loss: 0.8614 - val_acc: 0.6977\n",
      "An experiment done\n",
      "An experiment begin\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv_1 (Conv2D)                 (None, 32, 32, 64)   1792        input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32, 32, 64)   256         conv_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 32, 32, 64)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 16, 16, 64)   0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_2 (Conv2D)                 (None, 16, 16, 128)  73856       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 16, 16, 128)  512         conv_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 16, 16, 128)  0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 16, 16, 1)    129         activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 16, 16, 128)  0           activation_2[0][0]               \n",
      "                                                                 conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 8, 8, 128)    0           multiply_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 8, 8, 256)    295168      max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 8, 8, 256)    1024        conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 8, 8, 256)    0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 256)          0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10)           2570        global_average_pooling2d_1[0][0] \n",
      "==================================================================================================\n",
      "Total params: 375,307\n",
      "Trainable params: 374,411\n",
      "Non-trainable params: 896\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 1.7376 - acc: 0.3732Epoch 00001: val_loss improved from inf to 1.67813, saving model to ./model_noise1_att1.h5\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 1.7370 - acc: 0.3735 - val_loss: 1.6781 - val_acc: 0.3666\n",
      "Epoch 2/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 1.5648 - acc: 0.4385Epoch 00002: val_loss improved from 1.67813 to 1.54493, saving model to ./model_noise1_att1.h5\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 1.5644 - acc: 0.4387 - val_loss: 1.5449 - val_acc: 0.4283\n",
      "Epoch 3/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 1.4939 - acc: 0.4657Epoch 00003: val_loss improved from 1.54493 to 1.47289, saving model to ./model_noise1_att1.h5\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 1.4936 - acc: 0.4658 - val_loss: 1.4729 - val_acc: 0.4704\n",
      "Epoch 4/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 1.4424 - acc: 0.4839Epoch 00004: val_loss improved from 1.47289 to 1.41734, saving model to ./model_noise1_att1.h5\n",
      "782/782 [==============================] - 31s 40ms/step - loss: 1.4422 - acc: 0.4841 - val_loss: 1.4173 - val_acc: 0.4805\n",
      "Epoch 5/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 1.4037 - acc: 0.5002Epoch 00005: val_loss improved from 1.41734 to 1.37605, saving model to ./model_noise1_att1.h5\n",
      "782/782 [==============================] - 31s 40ms/step - loss: 1.4038 - acc: 0.5001 - val_loss: 1.3761 - val_acc: 0.5091\n",
      "Epoch 6/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 1.3699 - acc: 0.5115Epoch 00006: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 1.3694 - acc: 0.5117 - val_loss: 1.3875 - val_acc: 0.4968\n",
      "Epoch 7/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 1.3455 - acc: 0.5199- ETA: 2s - loss: 1.3 - ETA: 1s - loss: 1Epoch 00007: val_loss improved from 1.37605 to 1.30400, saving model to ./model_noise1_att1.h5\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 1.3456 - acc: 0.5199 - val_loss: 1.3040 - val_acc: 0.5287\n",
      "Epoch 8/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 1.3213 - acc: 0.5327Epoch 00008: val_loss improved from 1.30400 to 1.29835, saving model to ./model_noise1_att1.h5\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 1.3215 - acc: 0.5326 - val_loss: 1.2983 - val_acc: 0.5255\n",
      "Epoch 9/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 1.3053 - acc: 0.5363Epoch 00009: val_loss did not improve\n",
      "782/782 [==============================] - 29s 37ms/step - loss: 1.3051 - acc: 0.5362 - val_loss: 1.3009 - val_acc: 0.5324\n",
      "Epoch 10/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 1.2875 - acc: 0.5430Epoch 00010: val_loss did not improve\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 1.2875 - acc: 0.5430 - val_loss: 1.3068 - val_acc: 0.5329\n",
      "Epoch 11/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "781/782 [============================>.] - ETA: 0s - loss: 1.2683 - acc: 0.5502Epoch 00011: val_loss improved from 1.29835 to 1.26480, saving model to ./model_noise1_att1.h5\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 1.2686 - acc: 0.5500 - val_loss: 1.2648 - val_acc: 0.5440\n",
      "Epoch 12/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 1.2535 - acc: 0.5559Epoch 00012: val_loss improved from 1.26480 to 1.25465, saving model to ./model_noise1_att1.h5\n",
      "782/782 [==============================] - 29s 38ms/step - loss: 1.2532 - acc: 0.5560 - val_loss: 1.2547 - val_acc: 0.5502\n",
      "Epoch 13/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 1.2372 - acc: 0.5631Epoch 00013: val_loss did not improve\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 1.2374 - acc: 0.5631 - val_loss: 1.4571 - val_acc: 0.4848\n",
      "Epoch 14/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 1.2270 - acc: 0.5669Epoch 00014: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 1.2269 - acc: 0.5670 - val_loss: 1.4103 - val_acc: 0.5185\n",
      "Epoch 15/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 1.2156 - acc: 0.5702Epoch 00015: val_loss improved from 1.25465 to 1.19199, saving model to ./model_noise1_att1.h5\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 1.2158 - acc: 0.5702 - val_loss: 1.1920 - val_acc: 0.5714\n",
      "Epoch 16/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 1.2015 - acc: 0.5733Epoch 00016: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 1.2017 - acc: 0.5732 - val_loss: 1.2300 - val_acc: 0.5633\n",
      "Epoch 17/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 1.1892 - acc: 0.5808Epoch 00017: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 1.1894 - acc: 0.5808 - val_loss: 1.2504 - val_acc: 0.5480\n",
      "Epoch 18/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 1.1843 - acc: 0.5810Epoch 00018: val_loss improved from 1.19199 to 1.18164, saving model to ./model_noise1_att1.h5\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 1.1847 - acc: 0.5809 - val_loss: 1.1816 - val_acc: 0.5820\n",
      "Epoch 19/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 1.1707 - acc: 0.5861Epoch 00019: val_loss improved from 1.18164 to 1.17398, saving model to ./model_noise1_att1.h5\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 1.1712 - acc: 0.5860 - val_loss: 1.1740 - val_acc: 0.5745\n",
      "Epoch 20/200\n",
      "778/782 [============================>.] - ETA: 0s - loss: 1.1611 - acc: 0.5887Epoch 00020: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 1.1608 - acc: 0.5888 - val_loss: 1.2627 - val_acc: 0.5470\n",
      "Epoch 21/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 1.1584 - acc: 0.5931- ETA: 0s - loss: 1.1574 - aEpoch 00021: val_loss did not improve\n",
      "782/782 [==============================] - 29s 38ms/step - loss: 1.1580 - acc: 0.5933 - val_loss: 1.2826 - val_acc: 0.5539\n",
      "Epoch 22/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 1.1446 - acc: 0.5958Epoch 00022: val_loss improved from 1.17398 to 1.17307, saving model to ./model_noise1_att1.h5\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 1.1447 - acc: 0.5956 - val_loss: 1.1731 - val_acc: 0.5760\n",
      "Epoch 23/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 1.1332 - acc: 0.5991Epoch 00023: val_loss improved from 1.17307 to 1.12467, saving model to ./model_noise1_att1.h5\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 1.1332 - acc: 0.5991 - val_loss: 1.1247 - val_acc: 0.5933\n",
      "Epoch 24/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 1.1307 - acc: 0.6028Epoch 00024: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 1.1307 - acc: 0.6027 - val_loss: 1.2630 - val_acc: 0.5540\n",
      "Epoch 25/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 1.1218 - acc: 0.6072Epoch 00025: val_loss did not improve\n",
      "782/782 [==============================] - 31s 40ms/step - loss: 1.1217 - acc: 0.6073 - val_loss: 1.1276 - val_acc: 0.5964\n",
      "Epoch 26/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 1.1153 - acc: 0.6110Epoch 00026: val_loss did not improve\n",
      "782/782 [==============================] - 31s 40ms/step - loss: 1.1155 - acc: 0.6109 - val_loss: 1.2144 - val_acc: 0.5663\n",
      "Epoch 27/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 1.1068 - acc: 0.6120Epoch 00027: val_loss improved from 1.12467 to 1.08890, saving model to ./model_noise1_att1.h5\n",
      "782/782 [==============================] - 31s 40ms/step - loss: 1.1060 - acc: 0.6125 - val_loss: 1.0889 - val_acc: 0.6178\n",
      "Epoch 28/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 1.1029 - acc: 0.6110Epoch 00028: val_loss did not improve\n",
      "782/782 [==============================] - 31s 40ms/step - loss: 1.1032 - acc: 0.6110 - val_loss: 1.2288 - val_acc: 0.5644\n",
      "Epoch 29/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 1.0934 - acc: 0.6179Epoch 00029: val_loss did not improve\n",
      "782/782 [==============================] - 29s 38ms/step - loss: 1.0935 - acc: 0.6179 - val_loss: 1.1885 - val_acc: 0.5773\n",
      "Epoch 30/200\n",
      "778/782 [============================>.] - ETA: 0s - loss: 1.0855 - acc: 0.6182- ETA:Epoch 00030: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 1.0857 - acc: 0.6183 - val_loss: 1.1537 - val_acc: 0.5874\n",
      "Epoch 31/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 1.0776 - acc: 0.6225- ETA:Epoch 00031: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 1.0776 - acc: 0.6225 - val_loss: 1.1653 - val_acc: 0.5937\n",
      "Epoch 32/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 1.0729 - acc: 0.6251Epoch 00032: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 1.0730 - acc: 0.6252 - val_loss: 1.1062 - val_acc: 0.6064\n",
      "Epoch 33/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 1.0663 - acc: 0.6251\n",
      "Epoch 00033: reducing learning rate to 4.999999873689376e-05.\n",
      "Epoch 00033: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 1.0662 - acc: 0.6251 - val_loss: 1.1498 - val_acc: 0.5955\n",
      "Epoch 34/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 1.0454 - acc: 0.6371Epoch 00034: val_loss improved from 1.08890 to 1.02278, saving model to ./model_noise1_att1.h5\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 1.0454 - acc: 0.6370 - val_loss: 1.0228 - val_acc: 0.6367\n",
      "Epoch 35/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 1.0399 - acc: 0.6377Epoch 00035: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 1.0398 - acc: 0.6378 - val_loss: 1.0489 - val_acc: 0.6353\n",
      "Epoch 36/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 1.0327 - acc: 0.6389Epoch 00036: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 1.0323 - acc: 0.6389 - val_loss: 1.0368 - val_acc: 0.6282\n",
      "Epoch 37/200\n",
      "778/782 [============================>.] - ETA: 0s - loss: 1.0327 - acc: 0.6396Epoch 00037: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 1.0332 - acc: 0.6394 - val_loss: 1.0252 - val_acc: 0.6393\n",
      "Epoch 38/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 1.0242 - acc: 0.6424Epoch 00038: val_loss did not improve\n",
      "782/782 [==============================] - 29s 37ms/step - loss: 1.0241 - acc: 0.6424 - val_loss: 1.0696 - val_acc: 0.6209\n",
      "Epoch 39/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 1.0258 - acc: 0.6417Epoch 00039: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 1.0259 - acc: 0.6416 - val_loss: 1.0281 - val_acc: 0.6297\n",
      "Epoch 40/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 1.0201 - acc: 0.6458- ETA: 0s - loss: 1.0209 - Epoch 00040: val_loss improved from 1.02278 to 1.01906, saving model to ./model_noise1_att1.h5\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 1.0199 - acc: 0.6460 - val_loss: 1.0191 - val_acc: 0.6379\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/200\n",
      "778/782 [============================>.] - ETA: 0s - loss: 1.0145 - acc: 0.6453- ETA: 2s - loEpoch 00041: val_loss did not improve\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 1.0145 - acc: 0.6454 - val_loss: 1.0328 - val_acc: 0.6339\n",
      "Epoch 42/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 1.0129 - acc: 0.6461Epoch 00042: val_loss improved from 1.01906 to 0.98488, saving model to ./model_noise1_att1.h5\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 1.0128 - acc: 0.6462 - val_loss: 0.9849 - val_acc: 0.6547\n",
      "Epoch 43/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 1.0095 - acc: 0.6488Epoch 00043: val_loss improved from 0.98488 to 0.96939, saving model to ./model_noise1_att1.h5\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 1.0097 - acc: 0.6487 - val_loss: 0.9694 - val_acc: 0.6595\n",
      "Epoch 44/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 1.0080 - acc: 0.6483Epoch 00044: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 1.0077 - acc: 0.6484 - val_loss: 1.0188 - val_acc: 0.6379\n",
      "Epoch 45/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 1.0000 - acc: 0.6519Epoch 00045: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.9999 - acc: 0.6520 - val_loss: 1.0364 - val_acc: 0.6325\n",
      "Epoch 46/200\n",
      "778/782 [============================>.] - ETA: 0s - loss: 1.0020 - acc: 0.6504Epoch 00046: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 1.0022 - acc: 0.6503 - val_loss: 1.0327 - val_acc: 0.6375\n",
      "Epoch 47/200\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.9969 - acc: 0.6527Epoch 00047: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.9969 - acc: 0.6526 - val_loss: 1.0164 - val_acc: 0.6424\n",
      "Epoch 48/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.9960 - acc: 0.6526Epoch 00048: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.9960 - acc: 0.6526 - val_loss: 1.0139 - val_acc: 0.6386\n",
      "Epoch 49/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.9890 - acc: 0.6544- ETA: 1s - loss: 0.988\n",
      "Epoch 00049: reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 00049: val_loss did not improve\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 0.9889 - acc: 0.6544 - val_loss: 1.0094 - val_acc: 0.6429\n",
      "Epoch 50/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.9766 - acc: 0.6612Epoch 00050: val_loss improved from 0.96939 to 0.95929, saving model to ./model_noise1_att1.h5\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 0.9768 - acc: 0.6611 - val_loss: 0.9593 - val_acc: 0.6658\n",
      "Epoch 51/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.9777 - acc: 0.6618Epoch 00051: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.9781 - acc: 0.6617 - val_loss: 0.9728 - val_acc: 0.6572\n",
      "Epoch 52/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.9723 - acc: 0.6617- ETA: 2s - loss: 0.9730 - acc: 0.66 - ETA:Epoch 00052: val_loss improved from 0.95929 to 0.95658, saving model to ./model_noise1_att1.h5\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.9722 - acc: 0.6618 - val_loss: 0.9566 - val_acc: 0.6650\n",
      "Epoch 53/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.9703 - acc: 0.6633Epoch 00053: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.9702 - acc: 0.6632 - val_loss: 0.9871 - val_acc: 0.6521\n",
      "Epoch 54/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.9729 - acc: 0.6644Epoch 00054: val_loss did not improve\n",
      "782/782 [==============================] - 29s 37ms/step - loss: 0.9727 - acc: 0.6644 - val_loss: 0.9666 - val_acc: 0.6594\n",
      "Epoch 55/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.9699 - acc: 0.6629- ETA: 5s - loss: 0.9698 - acc: 0. - ETA: 5s - loss: 0.9695 - acc: 0 - ETA: 1s - lEpoch 00055: val_loss improved from 0.95658 to 0.95157, saving model to ./model_noise1_att1.h5\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.9700 - acc: 0.6628 - val_loss: 0.9516 - val_acc: 0.6656\n",
      "Epoch 56/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.9713 - acc: 0.6632Epoch 00056: val_loss improved from 0.95157 to 0.94369, saving model to ./model_noise1_att1.h5\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.9713 - acc: 0.6633 - val_loss: 0.9437 - val_acc: 0.6673\n",
      "Epoch 57/200\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.9664 - acc: 0.6644Epoch 00057: val_loss improved from 0.94369 to 0.94124, saving model to ./model_noise1_att1.h5\n",
      "782/782 [==============================] - 29s 37ms/step - loss: 0.9666 - acc: 0.6644 - val_loss: 0.9412 - val_acc: 0.6677\n",
      "Epoch 58/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.9624 - acc: 0.6668Epoch 00058: val_loss improved from 0.94124 to 0.93657, saving model to ./model_noise1_att1.h5\n",
      "782/782 [==============================] - 29s 38ms/step - loss: 0.9624 - acc: 0.6668 - val_loss: 0.9366 - val_acc: 0.6709\n",
      "Epoch 59/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.9625 - acc: 0.6656Epoch 00059: val_loss did not improve\n",
      "782/782 [==============================] - 29s 37ms/step - loss: 0.9623 - acc: 0.6656 - val_loss: 0.9402 - val_acc: 0.6701\n",
      "Epoch 60/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.9611 - acc: 0.6682Epoch 00060: val_loss did not improve\n",
      "782/782 [==============================] - 28s 36ms/step - loss: 0.9612 - acc: 0.6681 - val_loss: 0.9712 - val_acc: 0.6633\n",
      "Epoch 61/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.9620 - acc: 0.6671Epoch 00061: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.9624 - acc: 0.6669 - val_loss: 0.9500 - val_acc: 0.6682\n",
      "Epoch 62/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.9603 - acc: 0.6668- ETA:Epoch 00062: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.9601 - acc: 0.6668 - val_loss: 0.9507 - val_acc: 0.6667\n",
      "Epoch 63/200\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.9621 - acc: 0.6654- ETA: 5Epoch 00063: val_loss did not improve\n",
      "782/782 [==============================] - 29s 37ms/step - loss: 0.9618 - acc: 0.6655 - val_loss: 0.9597 - val_acc: 0.6648\n",
      "Epoch 64/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.9555 - acc: 0.6690Epoch 00064: val_loss improved from 0.93657 to 0.92852, saving model to ./model_noise1_att1.h5\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.9554 - acc: 0.6690 - val_loss: 0.9285 - val_acc: 0.6745\n",
      "Epoch 65/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.9569 - acc: 0.6692Epoch 00065: val_loss did not improve\n",
      "782/782 [==============================] - 29s 38ms/step - loss: 0.9569 - acc: 0.6692 - val_loss: 0.9532 - val_acc: 0.6589\n",
      "Epoch 66/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.9530 - acc: 0.6683Epoch 00066: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.9529 - acc: 0.6684 - val_loss: 0.9446 - val_acc: 0.6694\n",
      "Epoch 67/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.9514 - acc: 0.6687Epoch 00067: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.9515 - acc: 0.6686 - val_loss: 0.9384 - val_acc: 0.6710\n",
      "Epoch 68/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.9508 - acc: 0.6703- ETA: 1s - loss: 0.9514 Epoch 00068: val_loss did not improve\n",
      "782/782 [==============================] - 31s 40ms/step - loss: 0.9510 - acc: 0.6702 - val_loss: 0.9561 - val_acc: 0.6607\n",
      "Epoch 69/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.9480 - acc: 0.6698Epoch 00069: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.9476 - acc: 0.6699 - val_loss: 0.9443 - val_acc: 0.6660\n",
      "Epoch 70/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "781/782 [============================>.] - ETA: 0s - loss: 0.9522 - acc: 0.6719\n",
      "Epoch 00070: reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 00070: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.9524 - acc: 0.6719 - val_loss: 0.9415 - val_acc: 0.6672\n",
      "Epoch 71/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.9435 - acc: 0.6731Epoch 00071: val_loss improved from 0.92852 to 0.91183, saving model to ./model_noise1_att1.h5\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.9437 - acc: 0.6730 - val_loss: 0.9118 - val_acc: 0.6818\n",
      "Epoch 72/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.9398 - acc: 0.6762- ETA: 1s - loss:Epoch 00072: val_loss did not improve\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 0.9399 - acc: 0.6761 - val_loss: 0.9295 - val_acc: 0.6744\n",
      "Epoch 73/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.9392 - acc: 0.6753Epoch 00073: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.9391 - acc: 0.6753 - val_loss: 0.9216 - val_acc: 0.6732\n",
      "Epoch 74/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.9366 - acc: 0.6764- ETA: 4s - loss: 0.9368 - a - ETA: 4s  - ETA: 1s - loss:Epoch 00074: val_loss did not improve\n",
      "782/782 [==============================] - 29s 38ms/step - loss: 0.9371 - acc: 0.6762 - val_loss: 0.9191 - val_acc: 0.6764\n",
      "Epoch 75/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.9336 - acc: 0.6786Epoch 00075: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.9339 - acc: 0.6785 - val_loss: 0.9130 - val_acc: 0.6801\n",
      "Epoch 76/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.9352 - acc: 0.6793Epoch 00076: val_loss did not improve\n",
      "782/782 [==============================] - 32s 41ms/step - loss: 0.9354 - acc: 0.6792 - val_loss: 0.9133 - val_acc: 0.6802\n",
      "Epoch 77/200\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.9346 - acc: 0.6773\n",
      "Epoch 00077: reducing learning rate to 6.24999984211172e-06.\n",
      "Epoch 00077: val_loss did not improve\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 0.9351 - acc: 0.6772 - val_loss: 0.9267 - val_acc: 0.6734\n",
      "Epoch 78/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.9312 - acc: 0.6771Epoch 00078: val_loss improved from 0.91183 to 0.90846, saving model to ./model_noise1_att1.h5\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.9315 - acc: 0.6770 - val_loss: 0.9085 - val_acc: 0.6793\n",
      "Epoch 79/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.9365 - acc: 0.6774Epoch 00079: val_loss improved from 0.90846 to 0.90794, saving model to ./model_noise1_att1.h5\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.9366 - acc: 0.6774 - val_loss: 0.9079 - val_acc: 0.6802\n",
      "Epoch 80/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.9298 - acc: 0.6774Epoch 00080: val_loss improved from 0.90794 to 0.90672, saving model to ./model_noise1_att1.h5\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.9296 - acc: 0.6775 - val_loss: 0.9067 - val_acc: 0.6819\n",
      "Epoch 81/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.9291 - acc: 0.6789Epoch 00081: val_loss improved from 0.90672 to 0.90511, saving model to ./model_noise1_att1.h5\n",
      "782/782 [==============================] - 29s 37ms/step - loss: 0.9292 - acc: 0.6788 - val_loss: 0.9051 - val_acc: 0.6834\n",
      "Epoch 82/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.9333 - acc: 0.6780Epoch 00082: val_loss did not improve\n",
      "782/782 [==============================] - 29s 38ms/step - loss: 0.9336 - acc: 0.6779 - val_loss: 0.9082 - val_acc: 0.6819\n",
      "Epoch 83/200\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.9315 - acc: 0.6789Epoch 00083: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.9315 - acc: 0.6788 - val_loss: 0.9059 - val_acc: 0.6819\n",
      "Epoch 84/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.9323 - acc: 0.6762Epoch 00084: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.9322 - acc: 0.6763 - val_loss: 0.9058 - val_acc: 0.6838\n",
      "Epoch 85/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.9297 - acc: 0.6781Epoch 00085: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.9298 - acc: 0.6781 - val_loss: 0.9064 - val_acc: 0.6838\n",
      "Epoch 86/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.9290 - acc: 0.6790Epoch 00086: val_loss improved from 0.90511 to 0.90404, saving model to ./model_noise1_att1.h5\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.9291 - acc: 0.6790 - val_loss: 0.9040 - val_acc: 0.6827\n",
      "Epoch 87/200\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.9283 - acc: 0.6797Epoch 00087: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.9288 - acc: 0.6796 - val_loss: 0.9080 - val_acc: 0.6807\n",
      "Epoch 88/200\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.9286 - acc: 0.6786Epoch 00088: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.9277 - acc: 0.6789 - val_loss: 0.9063 - val_acc: 0.6810\n",
      "Epoch 89/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.9283 - acc: 0.6786Epoch 00089: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.9287 - acc: 0.6784 - val_loss: 0.9051 - val_acc: 0.6808\n",
      "Epoch 90/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.9257 - acc: 0.6791Epoch 00090: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.9255 - acc: 0.6792 - val_loss: 0.9083 - val_acc: 0.6804\n",
      "Epoch 91/200\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.9275 - acc: 0.6810Epoch 00091: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.9276 - acc: 0.6809 - val_loss: 0.9051 - val_acc: 0.6818\n",
      "Epoch 92/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.9259 - acc: 0.6796\n",
      "Epoch 00092: reducing learning rate to 3.12499992105586e-06.\n",
      "Epoch 00092: val_loss did not improve\n",
      "782/782 [==============================] - 31s 40ms/step - loss: 0.9258 - acc: 0.6796 - val_loss: 0.9077 - val_acc: 0.6809\n",
      "Epoch 93/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.9260 - acc: 0.6821Epoch 00093: val_loss improved from 0.90404 to 0.90246, saving model to ./model_noise1_att1.h5\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 0.9264 - acc: 0.6820 - val_loss: 0.9025 - val_acc: 0.6830\n",
      "Epoch 94/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.9249 - acc: 0.6828Epoch 00094: val_loss improved from 0.90246 to 0.90208, saving model to ./model_noise1_att1.h5\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.9248 - acc: 0.6828 - val_loss: 0.9021 - val_acc: 0.6845\n",
      "Epoch 95/200\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.9275 - acc: 0.6793- ETA: 1s - losEpoch 00095: val_loss improved from 0.90208 to 0.90112, saving model to ./model_noise1_att1.h5\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.9275 - acc: 0.6792 - val_loss: 0.9011 - val_acc: 0.6841\n",
      "Epoch 96/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.9250 - acc: 0.6804- ETA: 0s - loss: 0.9249 - acc: 0.68Epoch 00096: val_loss improved from 0.90112 to 0.89952, saving model to ./model_noise1_att1.h5\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.9251 - acc: 0.6802 - val_loss: 0.8995 - val_acc: 0.6843\n",
      "Epoch 97/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.9251 - acc: 0.6805Epoch 00097: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.9250 - acc: 0.6804 - val_loss: 0.9014 - val_acc: 0.6812\n",
      "Epoch 98/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.9236 - acc: 0.6820Epoch 00098: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.9237 - acc: 0.6819 - val_loss: 0.9006 - val_acc: 0.6839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.9252 - acc: 0.6807Epoch 00099: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.9246 - acc: 0.6809 - val_loss: 0.9001 - val_acc: 0.6828\n",
      "Epoch 100/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.9254 - acc: 0.6803Epoch 00100: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.9255 - acc: 0.6803 - val_loss: 0.9008 - val_acc: 0.6807\n",
      "Epoch 101/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.9234 - acc: 0.6811Epoch 00101: val_loss did not improve\n",
      "782/782 [==============================] - 29s 37ms/step - loss: 0.9233 - acc: 0.6811 - val_loss: 0.8996 - val_acc: 0.6829\n",
      "Epoch 102/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.9244 - acc: 0.6788- ETA: 3s - loss: 0.9232 -\n",
      "Epoch 00102: reducing learning rate to 1.56249996052793e-06.\n",
      "Epoch 00102: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.9242 - acc: 0.6789 - val_loss: 0.9023 - val_acc: 0.6825\n",
      "Epoch 103/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.9262 - acc: 0.6794Epoch 00103: val_loss improved from 0.89952 to 0.89909, saving model to ./model_noise1_att1.h5\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.9262 - acc: 0.6794 - val_loss: 0.8991 - val_acc: 0.6827\n",
      "Epoch 104/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.9221 - acc: 0.6824Epoch 00104: val_loss improved from 0.89909 to 0.89898, saving model to ./model_noise1_att1.h5\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.9221 - acc: 0.6824 - val_loss: 0.8990 - val_acc: 0.6823\n",
      "Epoch 105/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.9232 - acc: 0.6812Epoch 00105: val_loss improved from 0.89898 to 0.89809, saving model to ./model_noise1_att1.h5\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.9232 - acc: 0.6813 - val_loss: 0.8981 - val_acc: 0.6835\n",
      "Epoch 106/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.9179 - acc: 0.6847Epoch 00106: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.9182 - acc: 0.6847 - val_loss: 0.8994 - val_acc: 0.6817\n",
      "Epoch 107/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.9207 - acc: 0.6831Epoch 00107: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.9206 - acc: 0.6831 - val_loss: 0.8987 - val_acc: 0.6821\n",
      "Epoch 108/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.9256 - acc: 0.6795Epoch 00108: val_loss did not improve\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 0.9256 - acc: 0.6796 - val_loss: 0.9000 - val_acc: 0.6812\n",
      "Epoch 109/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.9227 - acc: 0.6827Epoch 00109: val_loss did not improve\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 0.9226 - acc: 0.6826 - val_loss: 0.8981 - val_acc: 0.6826\n",
      "Epoch 110/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.9228 - acc: 0.6829- ETA: 2s - loss: 0Epoch 00110: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.9227 - acc: 0.6830 - val_loss: 0.8993 - val_acc: 0.6822\n",
      "Epoch 111/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.9223 - acc: 0.6825\n",
      "Epoch 00111: reducing learning rate to 7.81249980263965e-07.\n",
      "Epoch 00111: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.9223 - acc: 0.6825 - val_loss: 0.8981 - val_acc: 0.6830\n",
      "Epoch 112/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.9177 - acc: 0.6839Epoch 00112: val_loss did not improve\n",
      "782/782 [==============================] - 29s 37ms/step - loss: 0.9178 - acc: 0.6839 - val_loss: 0.8984 - val_acc: 0.6825\n",
      "Epoch 113/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.9199 - acc: 0.6842Epoch 00113: val_loss improved from 0.89809 to 0.89789, saving model to ./model_noise1_att1.h5\n",
      "782/782 [==============================] - 31s 40ms/step - loss: 0.9203 - acc: 0.6841 - val_loss: 0.8979 - val_acc: 0.6839\n",
      "Epoch 114/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.9202 - acc: 0.6824Epoch 00114: val_loss did not improve\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 0.9202 - acc: 0.6824 - val_loss: 0.8981 - val_acc: 0.6832\n",
      "Epoch 115/200\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.9220 - acc: 0.6813Epoch 00115: val_loss did not improve\n",
      "782/782 [==============================] - 29s 38ms/step - loss: 0.9217 - acc: 0.6813 - val_loss: 0.8982 - val_acc: 0.6831\n",
      "Epoch 116/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.9217 - acc: 0.6820Epoch 00116: val_loss did not improve\n",
      "782/782 [==============================] - 29s 37ms/step - loss: 0.9215 - acc: 0.6821 - val_loss: 0.8992 - val_acc: 0.6817\n",
      "Epoch 117/200\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.9193 - acc: 0.6842Epoch 00117: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.9197 - acc: 0.6841 - val_loss: 0.8987 - val_acc: 0.6834\n",
      "Epoch 118/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.9170 - acc: 0.6871Epoch 00118: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.9169 - acc: 0.6871 - val_loss: 0.8988 - val_acc: 0.6832\n",
      "Epoch 119/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.9228 - acc: 0.6830\n",
      "Epoch 00119: reducing learning rate to 3.906249901319825e-07.\n",
      "Epoch 00119: val_loss did not improve\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 0.9228 - acc: 0.6829 - val_loss: 0.8985 - val_acc: 0.6837\n",
      "Epoch 120/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.9198 - acc: 0.6820- ETA: 1s - loss: 0.9194 Epoch 00120: val_loss improved from 0.89789 to 0.89775, saving model to ./model_noise1_att1.h5\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.9201 - acc: 0.6819 - val_loss: 0.8978 - val_acc: 0.6844\n",
      "Epoch 121/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.9181 - acc: 0.6827Epoch 00121: val_loss did not improve\n",
      "782/782 [==============================] - 29s 37ms/step - loss: 0.9185 - acc: 0.6826 - val_loss: 0.8982 - val_acc: 0.6842\n",
      "Epoch 122/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.9219 - acc: 0.6805- Epoch 00122: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.9219 - acc: 0.6806 - val_loss: 0.8978 - val_acc: 0.6838\n",
      "Epoch 123/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.9189 - acc: 0.6842Epoch 00123: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.9189 - acc: 0.6841 - val_loss: 0.8980 - val_acc: 0.6822\n",
      "Epoch 124/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.9164 - acc: 0.6838- ETA: 1s - loss: 0.9178 Epoch 00124: val_loss did not improve\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.9165 - acc: 0.6837 - val_loss: 0.8981 - val_acc: 0.6824\n",
      "Epoch 125/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.9194 - acc: 0.6845Epoch 00125: val_loss did not improve\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.9193 - acc: 0.6846 - val_loss: 0.8981 - val_acc: 0.6833\n",
      "Epoch 126/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.9225 - acc: 0.6805- ETA: 1s - loss: 0.9230 \n",
      "Epoch 00126: reducing learning rate to 1.9531249506599124e-07.\n",
      "Epoch 00126: val_loss did not improve\n",
      "782/782 [==============================] - 29s 37ms/step - loss: 0.9224 - acc: 0.6805 - val_loss: 0.8978 - val_acc: 0.6836\n",
      "Epoch 127/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.9192 - acc: 0.6844Epoch 00127: val_loss did not improve\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 0.9192 - acc: 0.6842 - val_loss: 0.8983 - val_acc: 0.6837\n",
      "Epoch 128/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.9225 - acc: 0.6816- ETA: 1s - loss: 0.9212Epoch 00128: val_loss did not improve\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 0.9225 - acc: 0.6816 - val_loss: 0.8985 - val_acc: 0.6834\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 129/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.9218 - acc: 0.6809Epoch 00129: val_loss did not improve\n",
      "782/782 [==============================] - 29s 38ms/step - loss: 0.9219 - acc: 0.6809 - val_loss: 0.8979 - val_acc: 0.6832\n",
      "Epoch 130/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.9218 - acc: 0.6822- ETA: 0s - loss: 0.9222 - acc: 0.6Epoch 00130: val_loss did not improve\n",
      "782/782 [==============================] - 29s 37ms/step - loss: 0.9216 - acc: 0.6822 - val_loss: 0.8981 - val_acc: 0.6832\n",
      "An experiment done\n",
      "All experiments done\n"
     ]
    }
   ],
   "source": [
    "# w/o noise, w/o attention\n",
    "# w/o noise, w attention\n",
    "# w noise, w/o attention\n",
    "# w noise, w attention\n",
    "Experiment_list = [(0, 0), (0, 1), (1, 0), (1, 1)]\n",
    "train_loss, train_acc, valid_loss, valid_acc = [], [], [], []\n",
    "\n",
    "for exp_noise, exp_att in Experiment_list:\n",
    "    # The setting\n",
    "    print(\"An experiment begin\")\n",
    "    X_train, X_test, y_train, y_test = get_data(add_noise=exp_noise)\n",
    "    use_att = exp_att\n",
    "    \n",
    "    # Clear and build model\n",
    "    K.clear_session()\n",
    "    model = build_model(use_att)\n",
    "    model.summary()\n",
    "    \n",
    "    # Simple data generator and callbacks\n",
    "    datagen = ImageDataGenerator(\n",
    "        rotation_range=15,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "    \n",
    "    reduce_lr = ReduceLROnPlateau(patience=4, factor=0.5, verbose=1)\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=11)\n",
    "    m_name = \"./\" + \"model_noise\" + str(exp_noise) + \"_att\" + str(exp_att) + \".h5\"\n",
    "    model_ckpt = ModelCheckpoint(filepath=m_name, monitor='val_loss',save_best_only=True, verbose=1)\n",
    "    \n",
    "    optim = Adam(lr=1e-4)\n",
    "    model.compile(loss = 'categorical_crossentropy', \n",
    "                  optimizer = optim, \n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    model_history = model.fit_generator(datagen.flow(X_train, y_train, batch_size=128), \n",
    "                                        workers = 6,\n",
    "                                        epochs=200,\n",
    "                                        validation_data=(X_test,y_test), \n",
    "                                        callbacks=[reduce_lr, model_ckpt, early_stop])\n",
    "    \n",
    "    t_loss = model_history.history.get('loss')\n",
    "    t_acc = model_history.history.get('acc')\n",
    "    v_loss = model_history.history.get('val_loss')\n",
    "    v_acc = model_history.history.get('val_acc')\n",
    "    \n",
    "    train_loss.append(t_loss)\n",
    "    train_acc.append(t_acc)\n",
    "    valid_loss.append(v_loss)\n",
    "    valid_acc.append(v_acc)\n",
    "    \n",
    "    print(\"An experiment done\")\n",
    "\n",
    "print(\"All experiments done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfUAAAEICAYAAABGRG3WAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ncO3jz94AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3ab097a9b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfUAAAEICAYAAABGRG3WAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n1EVERHKESl1ERCRH/H/kgI2DCQ2ibgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3ab0993160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfUAAAEICAYAAABGRG3WAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMEIQURcEQRCEEcL/B5E3XbKsaEi5AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3aabc69f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfUAAAEICAYAAABGRG3WAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3ab0686860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_exp = len(Experiment_list)\n",
    "Expname = ['w/o noise, w/o att', 'w/o noise, w/att', 'w noise, w/o att', 'w noise, w att']\n",
    "for idx in np.arange(n_exp):\n",
    "    plt.plot(np.arange(len(train_loss[idx])), train_loss[idx], label = Expname[idx])\n",
    "plt.legend(loc = 'upper left', bbox_to_anchor = (1, 1.05))\n",
    "plt.title('training loss')\n",
    "plt.show()\n",
    "    \n",
    "for idx in np.arange(n_exp):\n",
    "    plt.plot(np.arange(len(train_acc[idx])), train_acc[idx], label = Expname[idx])\n",
    "plt.legend(loc = 'upper left', bbox_to_anchor = (1, 1.05))\n",
    "plt.title('training accuracy')\n",
    "plt.show()\n",
    "\n",
    "for idx in np.arange(n_exp):\n",
    "    plt.plot(np.arange(len(valid_loss[idx])), valid_loss[idx], label = Expname[idx])\n",
    "plt.legend(loc = 'upper left', bbox_to_anchor = (1, 1.05))\n",
    "plt.title('validation loss')\n",
    "plt.show()\n",
    "\n",
    "for idx in np.arange(n_exp):\n",
    "    plt.plot(np.arange(len(valid_acc[idx])), valid_acc[idx], label = Expname[idx])\n",
    "plt.legend(loc = 'upper left', bbox_to_anchor = (1, 1.05))\n",
    "plt.title('validation accuracy')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Backup\n",
    "## Simple case\n",
    "K.clear_session()\n",
    "use_att = False\n",
    "\n",
    "X_train, X_test, y_train, y_test = get_data(add_noise=True)\n",
    "model = build_model(use_att)\n",
    "model.summary()\n",
    "\n",
    "optim = Adam(lr=1e-4)\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = optim, metrics=['accuracy'])\n",
    "\n",
    "\"\"\"\n",
    "model_history = model.fit(X_train, y_train, \n",
    "                          epochs=100,\n",
    "                          batch_size=64,\n",
    "                          validation_data=(X_test,y_test)\n",
    "                          )\n",
    "\"\"\"\n",
    "model_history = model.fit_generator(datagen.flow(X_train, y_train, batch_size=64), \n",
    "                                    workers = 6,\n",
    "                                    epochs=100,\n",
    "                                    validation_data=(X_test,y_test), \n",
    "                                    callbacks=[reduce_lr])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST: Test acc: 0.9975"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
