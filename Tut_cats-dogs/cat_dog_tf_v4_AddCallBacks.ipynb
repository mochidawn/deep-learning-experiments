{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=16, do_augment=True, epochs=50, gpu_id=0, image_dir='dataset', image_size=(120, 120, 3), is_training=1, lr=0.0001, model_name='model.ckpt', n_classes=2, save_dir='./result', train_ratio=0.9, use_model_ckpt=None)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "#\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import subprocess\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm # if use notebook\n",
    "# from tqdm import tqdm, trange\n",
    "\n",
    "from threading import Thread, Event\n",
    "import queue\n",
    "\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "#from general_function_handler import op_img, call_generator\n",
    "#from model import create_graph, losses, evaluation, trainning\n",
    "#import model_2\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--gpu_id', default=0)\n",
    "parser.add_argument('--image_dir', default=\"dataset\")\n",
    "parser.add_argument('--save_dir', default='./result')\n",
    "parser.add_argument('--model_name', default = 'model.ckpt', type=str)\n",
    "parser.add_argument('--is_training', default=1, type=int)\n",
    "parser.add_argument('--batch_size', default=16, type=int)\n",
    "parser.add_argument('--do_augment', default=True, type = bool)\n",
    "parser.add_argument('--epochs', default=50, type=int)\n",
    "parser.add_argument('--lr', default=0.0001, type=float)\n",
    "parser.add_argument('--image_size', default=(120,120,3), type = int)\n",
    "parser.add_argument('--n_classes', default=2, type = int)\n",
    "parser.add_argument('--train_ratio', default=0.9, type = float)\n",
    "parser.add_argument('--use_model_ckpt', default = None, type = str)\n",
    "FLAGS = parser.parse_args([])\n",
    "print(FLAGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(FLAGS.gpu_id)\n",
    "import tensorflow as tf\n",
    "\n",
    "if not os.path.exists(FLAGS.save_dir):\n",
    "    os.makedirs(FLAGS.save_dir)\n",
    "\n",
    "model_graph_name = FLAGS.save_dir + FLAGS.model_name\n",
    "    \n",
    "graphs_dir = FLAGS.save_dir + '/graphs'\n",
    "if not os.path.exists(graphs_dir):\n",
    "    os.makedirs(graphs_dir)\n",
    "\n",
    "\"\"\"  Get data \"\"\"\n",
    "d_train = FLAGS.image_dir + '/train/'\n",
    "d_test = FLAGS.image_dir + '/test1/'\n",
    "\n",
    "image_train_list = glob.glob(d_train + '*.jpg')\n",
    "image_test_list = glob.glob(d_test + '*.jpg')\n",
    "\n",
    "df_train = pd.DataFrame({'img_path': image_train_list})\n",
    "df_test = pd.DataFrame({'img_path': image_test_list})\n",
    "\n",
    "df_train['cate'] = df_train.img_path.apply(os.path.basename)\n",
    "df_train['cate'] = [i.split(\".\")[0] for i in list(df_train.cate)]\n",
    "df_train.cate = df_train.cate.replace({'dog': 0, 'cat': 1})\n",
    "\n",
    "nb_epoch = FLAGS.epochs\n",
    "\n",
    "df_train_0, df_val_0 = train_test_split(df_train[df_train['cate'] == 0], test_size = 1-FLAGS.train_ratio)\n",
    "df_train_1, df_val_1 = train_test_split(df_train[df_train['cate'] == 1], test_size = 1-FLAGS.train_ratio)\n",
    "\n",
    "df_val = pd.concat((df_val_0, df_val_1)).reset_index(drop = True)\n",
    "\n",
    "del df_val_0, df_val_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_path</th>\n",
       "      <th>cate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dataset/train\\cat.0.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dataset/train\\cat.1.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dataset/train\\cat.10.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dataset/train\\cat.100.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dataset/train\\cat.1000.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     img_path  cate\n",
       "0     dataset/train\\cat.0.jpg     1\n",
       "1     dataset/train\\cat.1.jpg     1\n",
       "2    dataset/train\\cat.10.jpg     1\n",
       "3   dataset/train\\cat.100.jpg     1\n",
       "4  dataset/train\\cat.1000.jpg     1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sometimes = lambda aug: iaa.Sometimes(0.5, aug)\n",
    "\n",
    "seq = iaa.Sequential([\n",
    "    iaa.Crop(px=(0, 16)), # crop images from each side by 0 to 16px (randomly chosen)\n",
    "    iaa.Fliplr(0.5), # horizontally flip 50% of the images\n",
    "    #iaa.GaussianBlur(sigma=(0, 3.0)), # blur images with a sigma of 0 to 3.0\n",
    "    sometimes(iaa.Affine(\n",
    "            scale = (0.8,1.2),\n",
    "            translate_percent = (-0.2, 0.2),\n",
    "            rotate = (-30, 30),\n",
    "            order = [0, 1],\n",
    "            #cval = (0,255),\n",
    "            mode = 'wrap'\n",
    "            ))\n",
    "])\n",
    "\n",
    "def cv_load_and_resize(x, is_training = True):\n",
    "    im_w, im_h, im_c = FLAGS.image_size\n",
    "    im = cv2.imread(x)\n",
    "    im = cv2.resize(im, (im_w, im_h))\n",
    "    if FLAGS.do_augment and is_training:\n",
    "        im = seq.augment_image(im)\n",
    "    return im\n",
    "    \n",
    "def get_evalute(dt):\n",
    "    images = np.array([cv_load_and_resize(i, is_training = False) for i in dt.img_path], dtype=np.float32) # don't do augmentation!\n",
    "    \n",
    "    \"\"\" do preprocessing here\"\"\"\n",
    "    ## If use resnet or other transfer learning\n",
    "    \"\"\" \"\"\"\n",
    "    y_out = np.array(dt['cate'])\n",
    "    y_out = tf.keras.utils.to_categorical(y_out, num_classes=FLAGS.n_classes)\n",
    "    return images, y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_train_idx(bz, idp_len_list):\n",
    "    \"\"\" Description \n",
    "    Input:\n",
    "    bz: batch size\n",
    "    idp_len_list: independent length list \n",
    "    \n",
    "    Output:\n",
    "    yield index list\n",
    "    \"\"\"\n",
    "    bz_per_class = bz // len(idp_len_list) # To know how many samples should be get for each class\n",
    "    batch_num = [x // bz_per_class for x in idp_len_list]\n",
    "    batch_nth = [0] * len(idp_len_list)\n",
    "    \n",
    "    select = [list(range(x)) for x in idp_len_list]\n",
    "    \n",
    "    for s in select:\n",
    "        random.shuffle(s)\n",
    "    \n",
    "    while True:\n",
    "        idxs = []\n",
    "        for i in range(len(idp_len_list)):\n",
    "            if batch_nth[i] >= batch_num[i]:\n",
    "                batch_nth[i] = 0\n",
    "                random.shuffle(select[i])\n",
    "            idx = select[i][batch_nth[i] * bz_per_class: (batch_nth[i] + 1) * bz_per_class]\n",
    "            batch_nth[i] += 1\n",
    "            idxs.append(idx)\n",
    "        yield idxs\n",
    "    \n",
    "def get_train_data(queue, df_list):\n",
    "    while True:\n",
    "        idxs = queue.get()\n",
    "        \n",
    "        select_list = []\n",
    "        \n",
    "        for df, idx in zip(df_list, idxs):\n",
    "            select_list.append(df.iloc[idx])\n",
    "        select_list = pd.concat(select_list)\n",
    "        #print(select_list)\n",
    "        images = np.array([cv_load_and_resize(iid) for iid in select_list.img_path], dtype=np.float32)\n",
    "        images = images.astype(np.float32)\n",
    "        \"\"\" do preprocessing here\"\"\"\n",
    "        ## If use resnet or other transfer learning\n",
    "        \n",
    "        \"\"\" Y out \"\"\"\n",
    "        y_out = np.array(select_list['cate'])\n",
    "        y_out = tf.keras.utils.to_categorical(y_out, FLAGS.n_classes)\n",
    "        \n",
    "        yield images, y_out\n",
    "\n",
    "        \n",
    "def enqueue(queue, stop, gen_func, args):\n",
    "    gen = gen_func(*args)\n",
    "    while True:\n",
    "        if stop.is_set():\n",
    "            return\n",
    "        queue.put(next(gen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_queue = queue.Queue(maxsize=5)\n",
    "train_idx_queue = queue.Queue(maxsize=50)\n",
    "\n",
    "### to stop threads after training\n",
    "events = []\n",
    "\n",
    "### enqueue training index \n",
    "event = Event()\n",
    "thread = Thread(target=enqueue, \n",
    "                args= (train_idx_queue, \n",
    "                       event, \n",
    "                       get_train_idx, \n",
    "                       (FLAGS.batch_size, [len(df_train_0), len(df_train_1)])\n",
    "                                      ))\n",
    "thread.start()\n",
    "events.append(event)\n",
    "\n",
    "### enqueue train batch\n",
    "for i in range(6):\n",
    "    event = Event()\n",
    "    thread = Thread(target=enqueue, \n",
    "                    args = (train_queue,\n",
    "                            event, \n",
    "                            get_train_data, \n",
    "                            (train_idx_queue, [df_train_0, df_train_1])\n",
    "                   ))\n",
    "    thread.start()\n",
    "    events.append(event)\n",
    "\n",
    "a,b = train_queue.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2500, 120, 120, 3)\n",
      "(16, 120, 120, 3) (16, 2)\n"
     ]
    }
   ],
   "source": [
    "x_val, y_val = get_evalute(df_val)\n",
    "print(x_val.shape)\n",
    "print(a.shape, b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class EarlyStopping():\n",
    "    def __init__(self, patience, min_delta = 0.0001):\n",
    "        # validation loss should at least be less than current min_loss - min_delta\n",
    "        self.min_delta = min_delta \n",
    "        self.patience = patience\n",
    "        self.epoch_count = 0\n",
    "        self.min_loss = None\n",
    "        self.stop = False\n",
    "        \n",
    "    def on_epoch_end(self, val_loss, *args, **kwargs):\n",
    "        if self.min_loss is None or val_loss < self.min_loss - self.min_delta:\n",
    "            self.min_loss = val_loss\n",
    "            self.epoch_count = 0\n",
    "        else:\n",
    "            self.epoch_count += 1\n",
    "            \n",
    "        # if cumulative counts is larger than our patience, set the stop signal to True\n",
    "        if self.epoch_count >= self.patience:\n",
    "            self.stop = True\n",
    "        \n",
    "class Model_checkpoint():\n",
    "    def __init__(self, model_name, save_best_only = True):\n",
    "        self.min_loss = None\n",
    "        self.model_name = model_name\n",
    "        self.save_best_only = save_best_only\n",
    "        \n",
    "    def on_epoch_end(self, val_loss, nth_epoch, saver, sess, *args, **kwargs):\n",
    "        if self.min_loss is None or val_loss < self.min_loss:\n",
    "            self.min_loss = val_loss\n",
    "            saver.save(sess, \n",
    "                       self.model_name + '.ckpt')\n",
    "        if not save_best_only:\n",
    "            saver.save(sess, \n",
    "                       self.model_name + '_' + str(nth_epoch) + '.ckpt',\n",
    "                       global_step=nth_epoch)\n",
    "        \n",
    "class ReduceLROnPlateau():\n",
    "    def __init__(self, lr, factor, patience, min_lr = 1e-10):\n",
    "        self.lr = lr\n",
    "        self.factor = factor\n",
    "        self.patience = patience\n",
    "        self.min_lr = min_lr\n",
    "        self.min_loss = None\n",
    "        self.epoch_count = 0\n",
    "    \n",
    "    def on_epoch_end(self, val_loss, *args, **kwargs):\n",
    "        if self.min_loss is None or val_loss < self.min_loss:\n",
    "            epoch_count = 0\n",
    "            self.min_loss = val_loss\n",
    "        else:\n",
    "            self.epoch_count += 1\n",
    "        \n",
    "        if self.epoch_count == self.patience:\n",
    "            self.lr *= self.factor\n",
    "            self.epoch_count = 0\n",
    "            \n",
    "            if self.lr <= self.min_lr:\n",
    "                self.lr = self.min_lr\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "im_w, im_h, im_c = FLAGS.image_size\n",
    "\n",
    "global_step = tf.Variable(0, name='global_step',trainable=False)\n",
    "is_training = tf.get_variable('is_training', [], trainable=False)\n",
    "\n",
    "drp_holder = tf.placeholder(tf.float32)\n",
    "\n",
    "a_in = tf.keras.layers.Input(shape = (im_w, im_h, im_c))\n",
    "\n",
    "x = tf.keras.layers.Conv2D(kernel_size=(3,3), filters=32, activation=tf.nn.selu)(a_in)\n",
    "x = tf.keras.layers.MaxPooling2D(pool_size=(2,2))(x)\n",
    "\n",
    "x = tf.keras.layers.Conv2D(kernel_size=(3,3), filters=32, activation=tf.nn.selu)(x)\n",
    "x = tf.keras.layers.MaxPooling2D(pool_size=(2,2))(x)\n",
    "\n",
    "x = tf.keras.layers.Conv2D(kernel_size=(3,3), filters=64, activation=tf.nn.selu)(x)\n",
    "x = tf.keras.layers.MaxPooling2D(pool_size=(2,2))(x)\n",
    "\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "\n",
    "x = tf.keras.layers.Dense(units=64, activation=tf.nn.selu)(x)\n",
    "x = tf.keras.layers.Dropout(drp_holder)(x)\n",
    "\n",
    "x = tf.keras.layers.Dense(units=32, activation=tf.nn.selu)(x)\n",
    "x = tf.keras.layers.Dropout(drp_holder)(x)\n",
    "\n",
    "out = tf.keras.layers.Dense(units=FLAGS.n_classes, activation='linear')(x) # softmax will be at loss part\n",
    "\n",
    "y_holder = tf.placeholder(tf.float32, shape=[None, FLAGS.n_classes])\n",
    "total_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_holder, logits=out))\n",
    "\n",
    "\n",
    "\"\"\" Control dynamic learning rate and BN updates \"\"\"\n",
    "lr_holder = tf.placeholder(dtype=tf.float16, shape=[], name='learning_rate_controller')\n",
    "optim = tf.train.AdamOptimizer(learning_rate=lr_holder)\n",
    "\n",
    "\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(update_ops):\n",
    "    optim_op = optim.minimize(total_loss, global_step=global_step) #, global_step=global_step_class\n",
    "\n",
    "#optim_op = optim.minimize(total_loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(out, 1), tf.argmax(y_holder, 1))\n",
    "accuracy_op = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Run_collected_functions():\n",
    "    def __init__(self, callback_dicts):\n",
    "        self.on_session_begin = callback_dicts['on_session_begin']\n",
    "        self.on_session_end = callback_dicts['on_session_end']\n",
    "        self.on_batch_begin = callback_dicts['on_batch_begin']\n",
    "        self.on_batch_end = callback_dicts['on_batch_end']\n",
    "        self.on_epoch_begin = callback_dicts['on_epoch_begin']\n",
    "        self.on_epoch_end = callback_dicts['on_epoch_end']\n",
    "        \n",
    "    def run_on_epoch_end(self, val_loss, nth_epoch = None, sess = None, saver = None):\n",
    "        for func in self.on_epoch_end:\n",
    "            getattr(func, 'on_epoch_end')(val_loss = val_loss,\n",
    "                                          nth_epoch = nth_epoch,\n",
    "                                          sess = sess,\n",
    "                                          saver = saver)\n",
    "        \n",
    "    def on_session_end(self, *args, **kwargs):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-13:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\vashi\\Anaconda3\\lib\\threading.py\", line 914, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\vashi\\Anaconda3\\lib\\site-packages\\tqdm\\_tqdm.py\", line 144, in run\n",
      "    for instance in self.tqdm_cls._instances:\n",
      "  File \"C:\\Users\\vashi\\Anaconda3\\lib\\_weakrefset.py\", line 60, in __iter__\n",
      "    for itemref in self.data:\n",
      "RuntimeError: Set changed size during iteration\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#n_batch = len(df_train) // FLAGS.batch_size + 1 # standard way - look all samples per epoch\n",
    "n_batch = 200 # evaluate per 200 steps\n",
    "\n",
    "# -------------------------- #\n",
    "global_train_loss, global_train_acc = [], []\n",
    "global_valid_loss, global_valid_acc = [], []\n",
    "early_stop = EarlyStopping(patience=10)\n",
    "model_checkpt = Model_checkpoint(model_name=model_graph_name, save_best_only=True)\n",
    "reduce_lr = ReduceLROnPlateau(lr=FLAGS.lr, factor=0.5, patience=3)\n",
    "\n",
    "callback_dict = {\n",
    "    'on_session_begin':[], # start of a session\n",
    "    'on_batch_begin':[], # start of a training batch\n",
    "    'on_batch_end':[], # end of a training batch\n",
    "    'on_epoch_begin':[], # start of a epoch\n",
    "    'on_epoch_end':[early_stop, reduce_lr], # end of a epoch\n",
    "    'on_session_end':[] # end of a session\n",
    "    }\n",
    "callback_manager = Run_collected_functions(callback_dict)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver = tf.train.Saver()\n",
    "    sess.run([tf.global_variables_initializer()])\n",
    "    epoch_bar = tqdm(range(FLAGS.epochs), \n",
    "                     desc = \"Train epoch\", \n",
    "                     unit = \"Epoch\")\n",
    "    for i in epoch_bar:\n",
    "        if i == 0:\n",
    "            epoch_bar.set_description(\"Training loss/acc: %.2f/%.2f ;Validation: %.2f/%.2f\" % \n",
    "                                  (0.0,0.0,0.0,0.0))\n",
    "        else:\n",
    "            epoch_bar.set_description(\"Training loss/acc: %.2f/%.2f ;Validation: %.2f/%.2f\" % \n",
    "                                  (global_train_loss[-1], global_train_acc[-1], global_valid_loss[-1], global_valid_acc[-1]))\n",
    "        \n",
    "        train_epoch_loss, train_epoch_acc = [], []\n",
    "        train_batch_bar = tqdm(range(n_batch), \n",
    "                               desc = \"Training batch\", \n",
    "                               unit = \"batch\", \n",
    "                               leave = False)\n",
    "        for j in train_batch_bar:\n",
    "            x_, y_ = train_queue.get()\n",
    "            \n",
    "            batch_loss, batch_acc, _ = sess.run([total_loss, accuracy_op, optim_op], \n",
    "                                                feed_dict = {a_in: x_, \n",
    "                                                            y_holder: y_, \n",
    "                                                            lr_holder: reduce_lr.lr,\n",
    "                                                            tf.keras.backend.learning_phase(): 1,\n",
    "                                                            #is_training: True,\n",
    "                                                            drp_holder: 0.1})\n",
    "            train_epoch_loss.append(batch_loss)\n",
    "            train_epoch_acc.append(batch_acc)\n",
    "            current_train_loss = np.mean(train_epoch_loss)\n",
    "            current_train_acc = np.mean(train_epoch_acc)\n",
    "            train_batch_bar.set_description('Training loss / acc: %.2f/%.2f' % (current_train_loss, current_train_acc))    \n",
    "        \n",
    "        global_train_loss.append(current_train_loss)\n",
    "        global_train_acc.append(current_train_acc)\n",
    "        \n",
    "        valid_epoch_loss, valid_epoch_acc = [], []\n",
    "        valid_batch_bar = tqdm(range(len(df_val) // FLAGS.batch_size + 1), \n",
    "                               desc = \"Valid batch\", \n",
    "                               unit = \"batch\", leave = False)\n",
    "        for j in valid_batch_bar:\n",
    "            this_val_loss, this_val_acc = sess.run([total_loss, accuracy_op], \n",
    "                                                       feed_dict = {a_in: x_val[j*FLAGS.batch_size : (j+1) * FLAGS.batch_size], \n",
    "                                                                    y_holder: y_val[j*FLAGS.batch_size : (j+1) * FLAGS.batch_size],\n",
    "                                                                    tf.keras.backend.learning_phase(): 0,\n",
    "                                                                    #is_training: False,\n",
    "                                                                    drp_holder: 0.0} )\n",
    "            valid_epoch_loss.append(this_val_loss)\n",
    "            valid_epoch_acc.append(this_val_acc)\n",
    "            valid_batch_bar.set_description('Validation loss / acc: %.2f/%.2f' % (this_val_loss, this_val_acc))\n",
    "        \n",
    "        current_valid_loss = np.mean(valid_epoch_loss)\n",
    "        current_valid_acc = np.mean(valid_epoch_acc)\n",
    "        \n",
    "        # \n",
    "        global_valid_loss.append(current_valid_loss)\n",
    "        global_valid_acc.append(current_valid_acc)\n",
    "        \n",
    "        callback_manager.run_on_epoch_end(val_loss = current_valid_loss,\n",
    "                                          sess = sess,\n",
    "                                          saver = saver,\n",
    "                                          nth_epoch = i)\n",
    "        if early_stop.stop:\n",
    "            print(\"EarlyStop!\")\n",
    "            break\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(range(len(global_train_loss)), global_train_loss, 'b', label = 'training loss')\n",
    "plt.plot(range(len(global_valid_loss)), global_valid_loss, 'r', label = 'validation loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(range(len(global_train_acc)), global_train_acc, 'b', label = 'training accuracy')\n",
    "plt.plot(range(len(global_valid_acc)), global_valid_acc, 'r', label = 'validation accuracy')\n",
    "plt.legend(loc = 4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# no drp\n",
    "#Training loss/acc: 0.48/0.77 ;Validation: 0.43/0.80: 100% 50/50 [13:22<00:00, 16.04s/Epoch]\n",
    "\n",
    "# with drp = 0.1\n",
    "#Training loss/acc: 0.53/0.74 ;Validation: 0.48/0.76: 100% 50/50 [13:27<00:00, 16.14s/Epoch]\n",
    "\n",
    "# with drp = 0.2\n",
    "#Training loss/acc: 0.56/0.70 ;Validation: 0.47/0.78: 100% 50/50 [13:38<00:00, 16.38s/Epoch]"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {
    "1026dc6b84fd461cafac01f7db475c1a": {
     "views": [
      {
       "cell_index": 10
      }
     ]
    },
    "55e091c378614a8f8d0dd213ae911222": {
     "views": [
      {
       "cell_index": 11
      }
     ]
    },
    "74e91fa598734b7eb73b9aeccd59c49d": {
     "views": [
      {
       "cell_index": 10
      }
     ]
    },
    "7fc157da4fc44887aca711624bd0654a": {
     "views": [
      {
       "cell_index": 10
      }
     ]
    },
    "82a2aa8ce0b14870bd98091780ffb99e": {
     "views": [
      {
       "cell_index": 10
      }
     ]
    },
    "bab2d32fc0f04b669ff635e6f0acce0e": {
     "views": [
      {
       "cell_index": 10
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
